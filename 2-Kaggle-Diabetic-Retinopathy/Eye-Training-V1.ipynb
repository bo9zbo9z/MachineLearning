{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl85J9Iqh4yQ"
   },
   "source": [
    "## Kaggle APTOS 2019 Diabetic Retinopathy Detection/Classification\n",
    "\n",
    "Link to competition: https://www.kaggle.com/c/aptos2019-blindness-detection\n",
    "\n",
    "This notebook was converted from my prior Kaggle notebook.  Migrated to TF 2.x and converted various methods to be more native TF.  (Mainly because of using graph/mapped methods.)\n",
    "\n",
    "You will need to run this notebook in the Kaggle environment or download/upload the training data to Google Colab environment.  The training files are large.  I downloaded and copied them to Google drive for training.\n",
    "\n",
    "Results are from training images, not from Kaggle scoring.  The Kaggle score will be lower.  I did not finish the notebook to submit and score the model.\n",
    "\n",
    "\n",
    "## Final Classification Report from Training images:\n",
    "\n",
    "Accuracy : 0.9030694668820679\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0    0.97842   0.97143   0.97491       140\n",
    "           1    0.85915   0.94574   0.90037       129\n",
    "           2    0.89076   0.75714   0.81853       140\n",
    "           3    0.84000   0.94382   0.88889        89\n",
    "           4    0.93277   0.91736   0.92500       121\n",
    "\n",
    "    accuracy                        0.90307       619\n",
    "   macro avg    0.90022   0.90710   0.90154       619\n",
    "weighted avg    0.90491   0.90307   0.90188       619\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEKO1KDFmFfW"
   },
   "source": [
    "### Processing for using Google Drive, Kaggle and normal includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37556,
     "status": "ok",
     "timestamp": 1588007479191,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "xjJySeIXh_md",
    "outputId": "d63a224e-eeeb-41a3-c140-b5c5a63586e5"
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# Google Collab specific stuff....\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "!ls \"/content/drive/My Drive\"\n",
    "\n",
    "USING_COLLAB = True\n",
    "%tensorflow_version 2.x\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46886,
     "status": "ok",
     "timestamp": 1588007488527,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "ZiSm49mbk_8Y",
    "outputId": "186ef78c-5b77-4ca1-c094-8f9df1e30fda"
   },
   "outputs": [],
   "source": [
    "# Upload your \"kaggle.json\" file that you created from your Kaggle Account tab\n",
    "# If you downloaded it, it would be in your \"Downloads\" directory\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49566,
     "status": "ok",
     "timestamp": 1588007491209,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "RhgWqzEhlAFe",
    "outputId": "17964647-0412-4ea3-f581-333c13d415f2"
   },
   "outputs": [],
   "source": [
    "# To start, install kaggle libs\n",
    "#!pip install -q kaggle\n",
    "\n",
    "# Workaround to install the newest version\n",
    "# https://stackoverflow.com/questions/58643979/google-colaboratory-use-kaggle-server-version-1-5-6-client-version-1-5-4-fai\n",
    "!pip install kaggle --upgrade --force-reinstall --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52905,
     "status": "ok",
     "timestamp": 1588007494553,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "qh7gieQ9lAOA",
    "outputId": "53dca055-3766-494a-e2d0-810c27c405d2"
   },
   "outputs": [],
   "source": [
    "# On your VM, create kaggle directory and modify access rights\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 278883,
     "status": "ok",
     "timestamp": 1588007720534,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "3G6y3a-DlAWL",
    "outputId": "eb607ccf-0d81-4be8-a1da-1231aa0a813e"
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions list\n",
    "# Takes about 4 mins to download\n",
    "!kaggle competitions download -c aptos2019-blindness-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjsZkgGklAho"
   },
   "outputs": [],
   "source": [
    "# Takes about 5 mins to unzip\n",
    "!unzip -uq aptos2019-blindness-detection.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582574,
     "status": "ok",
     "timestamp": 1588008024266,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "4EMPE8P5lBRk",
    "outputId": "8a8d6369-e26c-4add-c9b1-a13427e0cfa1"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REyf_6OtlSEd"
   },
   "outputs": [],
   "source": [
    "# Cleanup to add some space....\n",
    "!rm -r test_images\n",
    "!rm aptos2019-blindness-detection.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 585214,
     "status": "ok",
     "timestamp": 1588008026912,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "6w1an-l5h4yT",
    "outputId": "3b1bb01c-0c36-461b-854d-b95cb72b50dd"
   },
   "outputs": [],
   "source": [
    "# Setup sys.path to find MachineLearning lib directory\n",
    "\n",
    "try: USING_COLLAB\n",
    "except NameError: USING_COLLAB = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if \"MachineLearning\" in sys.path[0]:\n",
    "    pass\n",
    "else:\n",
    "    print(sys.path)\n",
    "    if USING_COLLAB:\n",
    "        sys.path.insert(0, '/content/drive/My Drive/GitHub/MachineLearning/lib')  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    else:\n",
    "        sys.path.insert(0, '/Users/john/Documents/GitHub/MachineLearning/lib')  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    \n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596004,
     "status": "ok",
     "timestamp": 1588008037705,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "csTt9CUvh4yZ",
    "outputId": "5a8d4214-bcd0-4e2d-d14b-b764cd7841fe"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, warnings, time, copy, csv, gc\n",
    "import numpy as np \n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(\"AUTOTUNE: \", AUTOTUNE)\n",
    "\n",
    "from TrainingUtils import *\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", \"(Possibly )?corrupt EXIF data\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aplx71Xjh4yg"
   },
   "source": [
    "## Examine and understand data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 595995,
     "status": "ok",
     "timestamp": 1588008037706,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "PMdwqph-h4yd",
    "outputId": "6421ec28-1a6b-460a-a26c-2153a83a00e7"
   },
   "outputs": [],
   "source": [
    "# GLOBALS/CONFIG ITEMS\n",
    "\n",
    "# Set root directory path to data\n",
    "if USING_COLLAB:\n",
    "    #ROOT_PATH = \"/content/drive/My Drive/ImageData/KaggleDiabeticRetinopathy/Data\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    ROOT_PATH = \"\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "else:\n",
    "    ROOT_PATH = \"\"\n",
    "        \n",
    "# Establish global dictionary\n",
    "parms = GlobalParms(MODEL_NAME=\"model-Eye-V02.h5\",\n",
    "                    ROOT_PATH=ROOT_PATH,\n",
    "                    TRAIN_DIR=\"train_images\",\n",
    "                    MODEL_PATH=\"/content/drive/My Drive/ImageData/KaggleDiabeticRetinopathy/Models\",\n",
    "                    NUM_CLASSES=5,\n",
    "                    CLASS_NAMES=['Normal', 'Moderate', 'Mild', 'Proliferative', 'Severe'],\n",
    "                    IMAGE_ROWS=224,\n",
    "                    IMAGE_COLS=224,\n",
    "                    IMAGE_CHANNELS=3,\n",
    "                    BATCH_SIZE=16,\n",
    "                    EPOCS=20,\n",
    "                    IMAGE_EXT=\".png\",\n",
    "                    FINAL_ACTIVATION='sigmoid',\n",
    "                    LOSS='binary_crossentropy',\n",
    "                    METRICS=['accuracy'])\n",
    "\n",
    "parms.print_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mhhvmqN7aFa"
   },
   "outputs": [],
   "source": [
    "# Simple helper method to display batches of images with labels....        \n",
    "def show_batch(image_batch, label_batch, number_to_show=25, r=5, c=5, print_shape=False):\n",
    "    show_number = min(number_to_show, parms.BATCH_SIZE)\n",
    "\n",
    "    if show_number < 8: #if small number, then change row, col and figure size\n",
    "        if parms.IMAGE_COLS > 64 or parms.IMAGE_ROWS > 64:\n",
    "            plt.figure(figsize=(25,25)) \n",
    "        else:\n",
    "            plt.figure(figsize=(10,10))  \n",
    "        r = 4\n",
    "        c = 2 \n",
    "    else:\n",
    "        plt.figure(figsize=(10,10))  \n",
    "\n",
    "    if show_number == 1:\n",
    "        image_batch = np.expand_dims(image_batch, axis=0)\n",
    "        label_batch = np.expand_dims(label_batch, axis=0)\n",
    "\n",
    "    for n in range(show_number):\n",
    "        if print_shape:\n",
    "            print(\"Image shape: {}  Max: {}  Min: {}\".format(image_batch[n].shape, np.max(image_batch[n]), np.min(image_batch[n])))\n",
    "        ax = plt.subplot(r,c,n+1)\n",
    "        cmap=\"gray\"\n",
    "        if len(image_batch[n].shape) == 3:\n",
    "            if image_batch[n].shape[2] == 3:\n",
    "                cmap=\"viridis\"\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(image_batch[n]), cmap=plt.get_cmap(cmap))\n",
    "        tmp = label_batch[n].numpy().astype(int).sum(axis=0) - 1\n",
    "        plt.title(parms.CLASS_NAMES[tmp])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHYSz76HTdkR"
   },
   "source": [
    "# Total Pipeline:\n",
    "\n",
    "Create training and validation Pandas dataframes:\n",
    "\n",
    "- Read training csv file and convert to Pandas -> add file_path and diagnosis_multi -> create additional training examples and a balanced sample -> split into train and val dataframes -> remove duplicates from val\n",
    "\n",
    "\n",
    "Create training and validation datasets:\n",
    "\n",
    "- For both: Use Panda dataframe to create dataset passing file_path and diagnosis_multi -> read & load image and create label -> remove any black boarders using midpoint and resize -> apply image_add_weighted to the image -> apply cache\n",
    "\n",
    "- For training only: After cache -> apply additional image augmentation, random rotate and random zoom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqvCJswiTsBb"
   },
   "source": [
    "### Load csv file\n",
    "\n",
    "- Load list of filenames and diagnosis\n",
    "- Perform initiall analysis on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596163,
     "status": "ok",
     "timestamp": 1588008037879,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "X5sTFRiKTo4o",
    "outputId": "e1378704-ef82-47d9-9ce0-90a419667867"
   },
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(os.path.join(parms.ROOT_PATH, \"train.csv\"))\n",
    "all_df[\"file_path\"] = parms.TRAIN_PATH + \"/\" + all_df[\"id_code\"] + \".png\"\n",
    "print(\"Training set is {}\".format(len(all_df)))\n",
    "all_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596161,
     "status": "ok",
     "timestamp": 1588008037879,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "i2uUGRY1Vpuk",
    "outputId": "3fbf35a4-b426-4b7d-d7c6-b15eecf250d1"
   },
   "outputs": [],
   "source": [
    "# Apply method to create the label, every row will be processed\n",
    "def build_label(x):\n",
    "    if x == 0:\n",
    "        return \"1, 0, 0, 0, 0\"\n",
    "    elif x == 1:\n",
    "        return \"1, 1, 0, 0, 0\"\n",
    "    elif x == 2:\n",
    "        return \"1, 1, 1, 0, 0\"\n",
    "    elif x == 3:\n",
    "        return \"1, 1, 1, 1, 0\"\n",
    "    else:\n",
    "        return \"1, 1, 1, 1, 1\"\n",
    "\n",
    "all_df['diagnosis_multi'] = all_df['diagnosis'].apply(build_label)\n",
    "all_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quB1LwLyFk2a"
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#0-1805, 1-370, 2-999, 3-193, 4-295\n",
    "\n",
    "# 1, 370 * 2 = 740\n",
    "all_df = pd.concat([all_df, all_df.loc[all_df[\"diagnosis\"] == 1]])\n",
    "\n",
    "# 3, 193 * 3 = 579\n",
    "only_3_df = all_df.loc[all_df[\"diagnosis\"] == 3]\n",
    "all_df = pd.concat([all_df, only_3_df])\n",
    "all_df = pd.concat([all_df, only_3_df])\n",
    "\n",
    "\n",
    "# 4, 295 * 3 = 885\n",
    "only_4_df = all_df.loc[all_df[\"diagnosis\"] == 4]\n",
    "all_df = pd.concat([all_df, only_4_df])\n",
    "all_df = pd.concat([all_df, only_4_df])\n",
    "\n",
    "all_df.reset_index(drop=True)\n",
    "\n",
    "# Select some number per group for training\n",
    "SAMPLES_PER_GROUP = 700\n",
    "balanced_all_df = all_df.groupby('diagnosis').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596858,
     "status": "ok",
     "timestamp": 1588008038581,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "W8pcITn7JC3j",
    "outputId": "e3a31003-54b2-4d1f-9c5b-623d57a5389c"
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "balanced_all_df['diagnosis'].hist()\n",
    "balanced_all_df['diagnosis'].value_counts()\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1R8KsrBkjgl"
   },
   "source": [
    "## Build an input pipeline using dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596855,
     "status": "ok",
     "timestamp": 1588008038581,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "2QHEnxC3btC-",
    "outputId": "3dd5045c-fa0f-4c5b-be15-414288ad54d1"
   },
   "outputs": [],
   "source": [
    "# Create training and validation dataframes\n",
    "train_df, valid_df = train_test_split(balanced_all_df, \n",
    "                                      test_size = 0.2,\n",
    "                                      stratify = balanced_all_df['diagnosis'])\n",
    "\n",
    "\n",
    "train_df = shuffle(train_df) # Shuffle\n",
    "\n",
    "# remove any duplicate images from validation\n",
    "valid_df = valid_df.sort_values('id_code')\n",
    "valid_df = valid_df.drop_duplicates(subset='id_code', keep='first')\n",
    "\n",
    "print('Training len: ', train_df.shape[0], \"  Validation len: \", valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596854,
     "status": "ok",
     "timestamp": 1588008038582,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "HBkyykpLDgnU",
    "outputId": "5f17647c-d1b6-4b4d-a680-1d18f59691ec"
   },
   "outputs": [],
   "source": [
    "# set lengths and steps\n",
    "train_len = len(train_df)\n",
    "val_len = len(valid_df)\n",
    "images_list_len = train_len + val_len\n",
    "\n",
    "steps_per_epoch = np.ceil(train_len // parms.BATCH_SIZE) # set step sizes based on train & batch\n",
    "validation_steps = np.ceil(val_len // parms.BATCH_SIZE) # set step sizes based on val & batch\n",
    "\n",
    "print(\"Total number: \", images_list_len, \"  Train number: \", train_len, \"  Val number: \", val_len)\n",
    "print(\"Steps/EPOC: \", steps_per_epoch, \"  Steps/Validation: \", validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596852,
     "status": "ok",
     "timestamp": 1588008038582,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "VG0XuAlE3toV",
    "outputId": "c61888fb-fa91-4f15-9083-929489f7a5c0"
   },
   "outputs": [],
   "source": [
    "# final check on the numbers, because we dropped duplicate validation, numbers may not be 20%\n",
    "print(train_df['diagnosis'].value_counts())\n",
    "print(valid_df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4YEMnjVWMSa"
   },
   "source": [
    "### Methods for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzIHTsEUcY2p"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Did not use this, but left it in notebook.  Makea the image a square by cropping from the smallest side\n",
    "# I used this in the old Kaggle notebook, but found that it removed too much details\n",
    "def smallest_side_center_crop(image: tf.Tensor) -> tf.Tensor:\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "    # if h == w, then zoom in a bit cause the image is square\n",
    "    smallest_side = tf.math.minimum(h, w)\n",
    "    offset_height = tf.cast(tf.math.floor((h - smallest_side) / 2), dtype=tf.int32)\n",
    "    offset_width =  tf.cast(tf.math.floor((w - smallest_side) / 2), dtype=tf.int32)\n",
    "    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, smallest_side, smallest_side)   \n",
    "    return image\n",
    "\"\"\"\n",
    "\n",
    "# Will remove black rows looking at the MIDPOINT ONLY, and then crop image\n",
    "def remove_black_boarder_from_midpoint(image, mask_threshold=0.1):\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "    mh = tf.cast(tf.math.floor(h/2), dtype=tf.dtypes.int32)\n",
    "    mw = tf.cast(tf.math.floor(w/2), dtype=tf.dtypes.int32)\n",
    "    \n",
    "    # Create mask based on some threshold\n",
    "    image_grey = tf.image.rgb_to_grayscale(image)    \n",
    "    image_grey = tf.where(image_grey < mask_threshold, 0, 1)\n",
    "\n",
    "    # Find first non-black, starting from the midpoint\n",
    "    left = tf.math.argmax(image_grey[mh,:], axis=0, output_type=tf.dtypes.int32)\n",
    "    top = tf.math.argmax(image_grey[:,mw], axis=0, output_type=tf.dtypes.int32)\n",
    "    # Rotate 180 to get the other non-black starting\n",
    "    image_grey = tf.image.rot90(image_grey, 2)\n",
    "    right = tf.math.argmax(image_grey[mh,:], axis=0, output_type=tf.dtypes.int32)\n",
    "    bottom = tf.math.argmax(image_grey[:,mw], axis=0, output_type=tf.dtypes.int32)\n",
    "\n",
    "    right = w - right\n",
    "    bottom = h - bottom\n",
    "\n",
    "    image = tf.image.crop_to_bounding_box(image, top[0], left[0], bottom[0]-top[0], right[0]-left[0]) \n",
    "    return image\n",
    "\n",
    "\n",
    "# Read, decode the image, convert to float\n",
    "def read_decode_image(file_path: tf.Tensor, label_string: tf.Tensor) -> tf.Tensor:\n",
    "    # load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    image = tf.image.decode_png(image, channels=parms.IMAGE_CHANNELS)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    image = tf.image.convert_image_dtype(image, parms.IMAGE_DTYPE)\n",
    "\n",
    "    # Build the label\n",
    "    b = tf.strings.split(label_string, sep=\",\")\n",
    "    label = tf.strings.to_number(b, tf.int32)\n",
    "\n",
    "    # For both training and validation, remove any black boarders\n",
    "    image = remove_black_boarder_from_midpoint(image)\n",
    "\n",
    "    # For both training and validation, resize\n",
    "    image = tf.image.resize(image, [parms.IMAGE_ROWS, parms.IMAGE_COLS])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# Augmentations for training dataset, done after cache\n",
    "def image_aug(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)) #0-4, 0/360, 90/180/270\n",
    "\n",
    "    #######################################################\n",
    "    # random zoom => use random crop + resize which will zoom the image\n",
    "    #######################################################\n",
    "    if tf.random.uniform(()) > 0.4:\n",
    "        w = parms.IMAGE_COLS\n",
    "        h = parms.IMAGE_ROWS\n",
    "        p = 0.85\n",
    "        image = tf.image.resize(tf.image.random_crop(image, (int(h*p), int(w*p), 3)), (h, w))\n",
    "    #######################################################\n",
    "\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "from skimage.filters import gaussian\n",
    "\"\"\"\n",
    "Ben Graham won the competition 5 years ago and his augmentation was widely used\n",
    "in the last competition.  He applied a weighted average to all images.\n",
    "https://github.com/btgraham/SparseConvNet/tree/kaggle_Diabetic_Retinopathy_competition\n",
    "\n",
    "The main approach used in this competition was to apply CV2.add_weighted.  \n",
    "This is an approximation of CV2.add_weighted.  CV2 gave me weird results as a \n",
    "graph operation, realy did not work....so I used skimage for the gaussian blur and \n",
    "native TF for weighted average.  This will bring out the features of the eye images \n",
    "to make them similar. (and runs under graph processing). Assume image is color \n",
    "and already scaled between 0 and 1.\n",
    "\n",
    "https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html\n",
    "Add weighted uses this formula: dst = src1*alpha + src2*beta + gamma, where src1 \n",
    "is the original image and src2 is a blurred image.\n",
    "\"\"\"\n",
    "def image_add_weighted(image, sigmaX=50, alpha=4.0, beta=-4.0, gamma=0.5):\n",
    "    image_blur = gaussian(image, sigma=sigmaX, multichannel=True)\n",
    "    image = tf.add(tf.multiply(image, alpha), tf.multiply(image_blur, beta))\n",
    "    image = tf.add(image, gamma)\n",
    "    image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "# pre-cache mapped method\n",
    "def process_train_pre_cache(file_path: tf.Tensor, label_string: tf.Tensor) -> tf.Tensor:\n",
    "    image, label = read_decode_image(file_path, label_string)\n",
    "\n",
    "    im_shape = image.shape\n",
    "    [image,] = tf.py_function(image_add_weighted, [image], [tf.float32])  #parms must be tensors\n",
    "    image.set_shape(im_shape)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# post-cache mapped method, does augmentation\n",
    "def process_train_post_cache(image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n",
    "    image = image_aug(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# method mapped to load val\n",
    "def process_val(file_path: tf.Tensor, label_string: tf.Tensor) -> tf.Tensor:\n",
    "    image, label = read_decode_image(file_path, label_string)\n",
    "\n",
    "    im_shape = image.shape\n",
    "    [image,] = tf.py_function(image_add_weighted, [image], [tf.float32])  #parms must be tensors\n",
    "    image.set_shape(im_shape)\n",
    "\n",
    "    return image, label\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhwlsx-48daB"
   },
   "source": [
    "### Training setup - build train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617059,
     "status": "ok",
     "timestamp": 1588008058795,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "lSrwCo5J7zWo",
    "outputId": "cbb712bd-cd43-457b-f133-c4699370a444"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[\"file_path\"].values,\n",
    "                                                    train_df[\"diagnosis_multi\"].values)\n",
    "                                                  )\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for file_path, label in train_dataset.take(2):\n",
    "    print(\"File path: \", file_path.numpy().decode(\"utf-8\"), \"  Label: \", label.numpy())\n",
    "\n",
    "# map training images to processing, includes any augmentation\n",
    "train_dataset = train_dataset.map(process_train_pre_cache, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, label in train_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Label: \", label.numpy())\n",
    "    some_image = image.numpy()\n",
    "    some_label = label.numpy()\n",
    "\n",
    "# Remove cache if running under Kaggle\n",
    "train_dataset = train_dataset.cache(\"./eye_train.tfcache\") \\\n",
    "                             .map(process_train_post_cache, num_parallel_calls=AUTOTUNE) \\\n",
    "                             .batch(parms.BATCH_SIZE) \\\n",
    "                             .prefetch(1) \\\n",
    "                             .repeat()\n",
    "\n",
    "# Show the images, execute this cell multiple times to see the images\n",
    "for image, label in train_dataset.take(1):\n",
    "    sample_image, sample_label = image, label\n",
    "show_batch(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoLDjUFh8YQ8"
   },
   "source": [
    "### Validation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617057,
     "status": "ok",
     "timestamp": 1588008058796,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "-TZM_MFp5rGA",
    "outputId": "5fdb30b0-4b03-4b15-cf86-d2f309bdbf74"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pd\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((valid_df[\"file_path\"].values,\n",
    "                                                  valid_df[\"diagnosis_multi\"].values)\n",
    "                                                 )\n",
    "\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for file_path, label in val_dataset.take(2):\n",
    "    print(\"File path: \", file_path.numpy().decode(\"utf-8\"), \"  Label: \", label.numpy())\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "val_dataset = val_dataset.map(process_val, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, label in val_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Label: \", label.numpy())\n",
    "    some_image = image.numpy()\n",
    "    some_label = label.numpy()\n",
    "\n",
    "# Remove cache if running under Kaggle\n",
    "val_dataset = val_dataset.cache(\"./eye_val.tfcache\") \\\n",
    "                         .batch(parms.BATCH_SIZE) \\\n",
    "                         .prefetch(1) \\\n",
    "                         .repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 624700,
     "status": "ok",
     "timestamp": 1588008066442,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "QrNPp8r2CCjo",
    "outputId": "b06715c4-a65f-480e-89f9-e93650bd0aec"
   },
   "outputs": [],
   "source": [
    "# Final check before model training.  I added a string of the mask non-zero counts - need to make sure the masks \n",
    "# were created ok.  (got bit by this one after a small change....)\n",
    "\n",
    "# Test Validation or Train by changing the dataset\n",
    "\n",
    "#for image, mask in train_dataset.take(1):\n",
    "for image, label in val_dataset.take(1):\n",
    "    show_batch(image, label)  # Will show all of the batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WboW5rmAh4yv"
   },
   "source": [
    "## Build and compile model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6KnOf_oh4yw"
   },
   "outputs": [],
   "source": [
    "# Create any call backs for training...These are the most common.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, min_lr=1e-6)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=8, verbose=1)\n",
    "checkpointer = ModelCheckpoint(parms.MODEL_PATH, monitor='eye_metric', verbose=1, mode=\"max\", save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 630937,
     "status": "ok",
     "timestamp": 1588008072685,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "Dg1IuTqEh4y0",
    "outputId": "1bba2d47-8cac-4b3a-82d3-3fc9670dc6c8"
   },
   "outputs": [],
   "source": [
    "# Create model and compile it\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D, Conv2DTranspose, Concatenate, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, Nadam, SGD\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
    "densenet = tf.keras.applications.DenseNet121(include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Simple metric to track improvements, can reduce the number of statements, left as is for readability \n",
    "# Just helps to know when to save the model, it will be higher than actual results, but I found it very helpful\n",
    "# than just using normal accuracy.\n",
    "def eye_metric(y_true, y_pred):\n",
    "    y_true_label = tf.cast(y_true > 0.5, dtype=y_true.dtype)\n",
    "    y_pred_label = tf.cast(y_pred > 0.5, dtype=y_true.dtype)\n",
    "    y_true_arg = tf.math.argmax(tf.reverse(y_true_label, axis=[0]))\n",
    "    y_pred_arg = tf.math.argmax(tf.reverse(y_pred_label, axis=[0]))\n",
    "    return tf.cast(y_true_arg == y_pred_arg, dtype=y_true.dtype)\n",
    "\n",
    "# Build and compile model.  I used this model before, did not adjust parms.\n",
    "# You can change to try different configurations.  (DO percentages, Dense layers, etc)\n",
    "def build_compile_model():\n",
    "    model = Sequential()\n",
    "    model.add(densenet)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=0.00005),  #0.00005 orig\n",
    "        metrics=['accuracy', eye_metric])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHJP9A8_lLnr"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 632279,
     "status": "ok",
     "timestamp": 1588008074030,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "s3-Dw2YGjra4",
    "outputId": "64d120b4-cbef-4a90-c3b5-371091396187"
   },
   "outputs": [],
   "source": [
    "# if you stop execution during training, a lock file may need to be removed\n",
    "!ls\n",
    "#!rm eye_val.tfcache_0.lockfile\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEbBKvlX7X04"
   },
   "outputs": [],
   "source": [
    "# Reload the model from prior run\n",
    "#model = load_model(parms.MODEL_PATH, custom_objects={\"eye_metric\": eye_metric})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "P_AE9vRvh4y8",
    "outputId": "7a5b007f-6122-4485-f782-e0380975ec1f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train model\n",
    "\n",
    "model = build_compile_model()\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=parms.EPOCS, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[reduce_lr, earlystopper, checkpointer] \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btOfnuWEh4y_"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.figure()\n",
    "history_df[['eye_metric', 'val_eye_metric']].plot(title=\"eye_metric\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "history_df[['loss', 'val_loss']].plot(title=\"Loss\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "history_df[['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COki5lZ0h4zG"
   },
   "source": [
    "## Validate model's predictions\n",
    "- Create actual_lables and predict_labels\n",
    "- Calculate Confusion Matrix & Accuracy\n",
    "- Display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkrhk6FOh4zC"
   },
   "outputs": [],
   "source": [
    "# Reload the model from prior run\n",
    "model = load_model(parms.MODEL_PATH, custom_objects={\"eye_metric\": eye_metric})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4T-oypFsh4zM"
   },
   "outputs": [],
   "source": [
    "# Easy to modify to predict the given test files that do not have a mask\n",
    "\n",
    "# Method to be applied to all testing images\n",
    "def process_test_image_id(image_id, file_path: tf.Tensor, label_string: tf.Tensor) -> tf.Tensor:\n",
    "    image, label = read_decode_image(file_path, label_string)\n",
    "\n",
    "    im_shape = image.shape\n",
    "    [image,] = tf.py_function(image_add_weighted, [image], [tf.float32])  #parms must be tensors\n",
    "    image.set_shape(im_shape)\n",
    " \n",
    "    return image_id, image, label   \n",
    "\n",
    "# Create Dataset from pd, could use validation or training\n",
    "test_df = shuffle(valid_df)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df[\"id_code\"].values,\n",
    "                                                   test_df[\"file_path\"].values,\n",
    "                                                   test_df[\"diagnosis_multi\"].values)\n",
    "                                                 )\n",
    "# Verify image paths were loaded\n",
    "for image_id, image, label in test_dataset.take(2):\n",
    "    print(image_id.numpy().decode(\"utf-8\"), label.numpy())\n",
    "\n",
    "# map training images to processing, includes any augmentation\n",
    "test_dataset = test_dataset.map(process_test_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image_id, image, label in test_dataset.take(1):\n",
    "    print(\"Image Id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Label: \", label.numpy())\n",
    "    some_image = image.numpy()\n",
    "    \n",
    "test_dataset = test_dataset.batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHhhlMogtEZB"
   },
   "outputs": [],
   "source": [
    "def predictions_using_dataset_eye(model_actual,\n",
    "                              dataset,\n",
    "                              steps,\n",
    "                              batch_size,\n",
    "                              create_bad_results_list=True):\n",
    "    \"\"\"\n",
    "      Uses generator to predict results.  Builds actual_labels, predict_labels\n",
    "      and predict_probabilities\n",
    "\n",
    "      Args:\n",
    "        model_actual : trained model to use for predictions\n",
    "        dataset : dataset\n",
    "        steps : number of batches to process\n",
    "        batch_size : size of batch\n",
    "        create_bad_results_list : bool default True.  Lets you trun on/off\n",
    "            the creation of the bad results lists.\n",
    "\n",
    "      Returns:\n",
    "        actual_labels : list of actual labels\n",
    "        predict_labels : list of predicted labels\n",
    "        predict_probabilities : list of predicted probability array\n",
    "        bad_results : list of bad results [actual_labels, predict_labels,\n",
    "                      predict_probabilities, image]\n",
    "    \"\"\"\n",
    "\n",
    "    bad_cnt = 0.0\n",
    "    good_cnt = 0.0\n",
    "    total_cnt = 0\n",
    "    actual_labels = []\n",
    "    predict_labels = []\n",
    "    predict_probabilities = []\n",
    "    bad_results = []\n",
    "\n",
    "    for image_id_batch, image_batch, label_batch in tqdm(dataset.take(steps)):\n",
    "        for j in range(batch_size):\n",
    "            image_id = image_id_batch[j]\n",
    "            image = image_batch[j]\n",
    "            label = label_batch[j]\n",
    "\n",
    "            total_cnt += 1\n",
    " \n",
    "            actual_label = label.numpy().astype(int).sum(axis=0) - 1\n",
    "            if actual_label < 0:\n",
    "                actual_label = 0\n",
    " \n",
    "            image = np.expand_dims(image, axis=0)\n",
    "\n",
    "            predict_probabilities_tmp = model_actual.predict(image)[0]\n",
    "            predict_probabilities_actual = predict_probabilities_tmp > 0.5\n",
    "\n",
    "            if np.count_nonzero(predict_probabilities_actual.astype(int)) == 0:  # if nothing scored, default to a normal eye...what else can you do :)\n",
    "                predict_label = 0\n",
    "            else:\n",
    "                predict_label = (parms.NUM_CLASSES - np.argmax(np.flip(predict_probabilities_actual))) - 1\n",
    "\n",
    "            #print(image_id.numpy(), label.numpy(), actual_label, predict_label, predict_probabilities_tmp)\n",
    "\n",
    "            actual_labels.append(actual_label)\n",
    "            predict_labels.append(predict_label)\n",
    "            predict_probabilities.append(predict_probabilities_tmp)\n",
    "\n",
    "            correct_flag = actual_label == predict_label\n",
    "            if correct_flag:\n",
    "                good_cnt = good_cnt + 1\n",
    "            else:\n",
    "                bad_cnt = bad_cnt + 1\n",
    "                if create_bad_results_list:\n",
    "                    bad_results.append([image_id,\n",
    "                                        [actual_label],\n",
    "                                        [predict_label],\n",
    "                                        predict_probabilities_tmp,\n",
    "                                        image])\n",
    "    print(\" \")\n",
    "    print(\"total: \", total_cnt, \"  Good: \", good_cnt, \"  Bad: \",\n",
    "          bad_cnt, \"  percent good: \", str(good_cnt/total_cnt))\n",
    "\n",
    "    return actual_labels, predict_labels, predict_probabilities, \\\n",
    "        bad_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dhe5cS3gtHLy"
   },
   "outputs": [],
   "source": [
    "steps = val_len\n",
    "steps = 20\n",
    "actual_labels, predict_labels, predict_probabilities, bad_results = predictions_using_dataset_eye(model,\n",
    "                              test_dataset,\n",
    "                              steps,\n",
    "                              1,\n",
    "                              create_bad_results_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IurHe_Uh4zO"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(actual_labels, predict_labels, parms.CLASS_NAMES, show_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdAXk58Zh4zR"
   },
   "outputs": [],
   "source": [
    "# Graph the results\n",
    "display_prediction_results(actual_labels, predict_labels, predict_probabilities, parms.NUM_CLASSES, parms.CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4e2xRnkGh4zU"
   },
   "outputs": [],
   "source": [
    "#Create a df from the bad results list, can save as csv or use for further analysis\n",
    "bad_results_df = pd.DataFrame(bad_results, columns =['image_id', 'actual', 'predict', 'prob', 'image'])\n",
    "bad_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUOdBq9zQll-"
   },
   "outputs": [],
   "source": [
    "# Last check, spot check some entries as needed....\n",
    "#all_df.loc[all_df['id_code'] == \"e07045d7c5f7\"]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eye-Training-V1.ipynb",
   "provenance": [
    {
     "file_id": "11uAUoq-UC0ftULnwk2LTbVryt3h-D3Q2",
     "timestamp": 1582044452297
    },
    {
     "file_id": "1KpHc6u2_eLqzTYla8bEH-SIPLGWtZ_vW",
     "timestamp": 1581035272578
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
