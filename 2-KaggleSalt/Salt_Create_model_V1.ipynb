{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_l5_1J3JHTR-"
   },
   "source": [
    "# Kaggle Salt Segmentation - Create Model From Scratch\n",
    "\n",
    "Link to competition: https://www.kaggle.com/c/tgs-salt-identification-challenge\n",
    "\n",
    "This notebook was converted from my prior Kaggle notebook.  Migrated to TF 2.x and converted various methods to be more native TF.  This will create a model from initial training examples.  \n",
    "\n",
    "- Images are png\n",
    "- Doubled the number of training images to give additional examples.  Augmentation was applied to all images to reduce the impact of having duplicates.\n",
    "- Training took about 60 epocs, final metrics were: loss: 0.0490 - lb_metric: 0.8261 - val_loss: 0.0962 - val_lb_metric: 0.7862\n",
    "- Validation scores from Training images: (224, 224):  0.803   (101, 101):  0.805\n",
    "- Running on Google Colab took about 40 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuG6og89HTSP"
   },
   "outputs": [],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "#\"\"\"\n",
    "# Google Collab specific stuff....\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "!ls \"/content/drive/My Drive\"\n",
    "\n",
    "USING_COLLAB = True\n",
    "%tensorflow_version 2.x\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSSL1xIugoNA"
   },
   "source": [
    "### Start Kaggle Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byXTF8SHHTSE"
   },
   "outputs": [],
   "source": [
    "# Upload your \"kaggle.json\" file that you created from your Kaggle Account tab\n",
    "# If you downloaded it, it would be in your \"Downloads\" directory\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mVWdxnml5tA"
   },
   "outputs": [],
   "source": [
    "# Double check to see what files already exist\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efcTa1ZMHTSI"
   },
   "outputs": [],
   "source": [
    "# On your VM, create kaggle directory and modify access rights\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfFm8MVJHTSA"
   },
   "outputs": [],
   "source": [
    "# Install kaggle libs\n",
    "!pip uninstall -y kaggle\n",
    "!pip install --upgrade pip\n",
    "!pip install kaggle==1.5.6\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xANlmUSpHTSL"
   },
   "outputs": [],
   "source": [
    "# Download salt files and unzip\n",
    "!kaggle competitions download -c tgs-salt-identification-challenge\n",
    "!ls\n",
    "!unzip -q tgs-salt-identification-challenge.zip\n",
    "!ls\n",
    "!unzip -q train.zip -d train\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWy5cp6bHTSO"
   },
   "source": [
    "### End Kaggle Data Install, Start Normal Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tXEKxvHHTSZ"
   },
   "outputs": [],
   "source": [
    "# Setup sys.path to find MachineLearning lib directory\n",
    "\n",
    "try: USING_COLLAB\n",
    "except NameError: USING_COLLAB = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if \"MachineLearning\" in sys.path[0]:\n",
    "    pass\n",
    "else:\n",
    "    print(sys.path)\n",
    "    if USING_COLLAB:\n",
    "        sys.path.insert(0, '/content/drive/My Drive/GitHub/MachineLearning/lib') ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    else:\n",
    "        sys.path.insert(0, '/Users/john/Documents/GitHub/MachineLearning/lib')  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    \n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsD601IAHTSd"
   },
   "outputs": [],
   "source": [
    "# Normal includes...\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, warnings, time, copy, csv\n",
    "import numpy as np \n",
    "\n",
    "import IPython.display as display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from TrainingUtils import *\n",
    "from losses.Losses_Babakhin import make_loss, Kaggle_IoU_Precision, dice_coef_loss_bce\n",
    "\n",
    "# This allows the runtime to decide how best to optimize CPU/GPU usage\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBFVcRd0HTSg"
   },
   "source": [
    "## Various Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kc3qYbjfHTSh"
   },
   "outputs": [],
   "source": [
    "# Set these to match your environment\n",
    "\n",
    "if USING_COLLAB:\n",
    "    ROOT_PATH = \"\"\n",
    "    MODEL_PATH = \"/content/drive/My Drive/ImageData/KaggleSaltDeposits/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "else:\n",
    "    ROOT_PATH = \"/Users/john/Documents/ImageData/KaggleSaltDeposits/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    MODEL_PATH = None\n",
    "        \n",
    "# Establish global dictionary\n",
    "parms = GlobalParms(ROOT_PATH=ROOT_PATH,\n",
    "                    MODEL_NAME=\"salt_Model_V01.h5\",\n",
    "                    TRAIN_DIR=\"train\", \n",
    "                    MODEL_PATH=MODEL_PATH,\n",
    "                    NUM_CLASSES=1,\n",
    "                    IMAGE_ROWS=128,\n",
    "                    IMAGE_COLS=128,\n",
    "                    IMAGE_CHANNELS=1,\n",
    "                    BATCH_SIZE=24,\n",
    "                    EPOCS=60,\n",
    "                    FINAL_ACTIVATION=\"sigmoid\",\n",
    "                    IMAGE_EXT=\".png\")\n",
    "\n",
    "parms.print_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDHKSm0THTSk"
   },
   "outputs": [],
   "source": [
    "# Helper method to display images and masks\n",
    "def show_batch_mask(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        #print(np.max(display_list[i]), np.min(display_list[i]))\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]),cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANVgrSscHTSn"
   },
   "source": [
    "## Create training and validation files\n",
    "\n",
    "The number of ships is not balanced and the size of some of the images are very small.  The approach can be changed if you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVeFAh8wHTSo"
   },
   "outputs": [],
   "source": [
    "# Load files and create dataframe\n",
    "image_path = os.path.join(parms.TRAIN_PATH, \"images\")\n",
    "all_files_tmp = np.array(os.listdir(image_path))\n",
    "all_files = []\n",
    "for image_id in all_files_tmp:\n",
    "    if image_id.endswith(parms.IMAGE_EXT):\n",
    "        all_files.append(image_id)\n",
    "\n",
    "print(\"All files: \", len(all_files), \" \", all_files[0])\n",
    "\n",
    "# modify to reduce the number of images processed, or comment out for full list\n",
    "all_files = all_files\n",
    "\n",
    "# Create df from files\n",
    "all_df = pd.DataFrame(all_files, columns =['ImageId']) \n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "colab": {},
    "colab_type": "code",
    "id": "DBXo0Rc2HTSs"
   },
   "outputs": [],
   "source": [
    "masks_path = os.path.join(parms.TRAIN_PATH, \"masks\")\n",
    "images_path = os.path.join(parms.TRAIN_PATH, \"images\")\n",
    "\n",
    "# Load mask size and use to guess about salt\n",
    "all_df['mask_size'] = all_df['ImageId'].map(lambda image_id: round(os.stat(os.path.join(masks_path, image_id)).st_size))\n",
    "all_df['has_salt'] = all_df['mask_size'].map(lambda x: 0 if x < 90 else 1)\n",
    "\n",
    "# load image size and guess about all black images\n",
    "all_df[\"image_size\"] = all_df['ImageId'].map(lambda image_id: round(os.stat(os.path.join(images_path, image_id)).st_size) )\n",
    "all_df[\"black_image\"] = all_df['image_size'].map(lambda x: 1 if x < 110 else 0)\n",
    "\n",
    "# Delete all black images\n",
    "blackimages = all_df[all_df['black_image'] == 1].index\n",
    "all_df.drop(blackimages , inplace=True)\n",
    "\n",
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGSgG5lbtD8k"
   },
   "outputs": [],
   "source": [
    "# Create more training images, will reduce later\n",
    "all_df = pd.concat([all_df, all_df])\n",
    "all_df = shuffle(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iw5Yut_Zb6qJ"
   },
   "outputs": [],
   "source": [
    "# Stratifing by image_size, my prior notebook used the number of white pixels, this was easier and gave a better spread\n",
    "all_df_cut = pd.cut(all_df[\"image_size\"], bins=[0, 8000, 9000, 10250, 12000, 100000])\n",
    "ax = all_df_cut.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(20,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMUzH5plb6xK"
   },
   "outputs": [],
   "source": [
    "# Create \"image_group\" column\n",
    "def group_by_image_size(x):\n",
    "    #0, 90,250,300,350, 100000]\n",
    "    if x < 8000:\n",
    "        return 0\n",
    "    elif x < 9000:\n",
    "        return 1\n",
    "    elif x < 10250:\n",
    "        return 2\n",
    "    elif x < 12000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "all_df['image_group'] = all_df['image_size'].apply(group_by_image_size)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-fAY9gYb64i"
   },
   "outputs": [],
   "source": [
    "# Create a subset of rows to balance training\n",
    "SAMPLES_PER_GROUP = 1400\n",
    "balanced_train_df = all_df.groupby('image_group').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "balanced_train_df['image_group'].hist(bins=balanced_train_df['image_group'].max()+1)\n",
    "print(balanced_train_df.shape[0], 'image_group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Btwk5P_1HTS_"
   },
   "outputs": [],
   "source": [
    "# Create training and validation lists from the balanced df\n",
    "train_df, valid_df = train_test_split(balanced_train_df, \n",
    "                 test_size = 0.2,\n",
    "                 stratify = balanced_train_df['image_group'])\n",
    "\n",
    "train_df = shuffle(train_df) # Shuffle since same image could be grouped\n",
    "print('Training: ', train_df.shape[0], '   Validation: ',valid_df.shape[0])\n",
    "\n",
    "# set lengths and steps\n",
    "train_len = len(train_df)\n",
    "val_len = len(valid_df)\n",
    "images_list_len = train_len + val_len\n",
    "\n",
    "steps_per_epoch = np.ceil(train_len // parms.BATCH_SIZE) # set step sizes based on train & batch\n",
    "validation_steps = np.ceil(val_len // parms.BATCH_SIZE) # set step sizes based on val & batch\n",
    "\n",
    "print(\"Total number: \", images_list_len, \"  Train number: \", train_len, \"  Val number: \", val_len)\n",
    "print(\"Steps/EPOC: \", steps_per_epoch, \"  Steps/Validation: \", validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoTg9jKwHTTH"
   },
   "source": [
    "## Build, load and augment TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9lTnHreHTTJ"
   },
   "outputs": [],
   "source": [
    "# Image augmentations\n",
    "def image_mask_aug(image, mask):\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.25:    \n",
    "        k = tf.random.uniform(shape=[], minval=1, maxval=4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k) #0-4, 0/270, 90/180/270\n",
    "        mask = tf.image.rot90(mask, k) #0-4, 0/270, 90/180/270\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "        \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        mask = tf.image.flip_up_down(mask)\n",
    "\n",
    "    # Really rough way to adjust the image, larger gamma => darken image, smaller gamma => lightens image\n",
    "    # so used image mean and add 0.5 to get values between 05 and 1.5, apply those\n",
    "    # using adjust_gamma.  (image mean of 0 would be a black image, mean of 255 would be a white image)\n",
    "    # If applied to all, then should also apply to testing data\n",
    "    gamma = tf.math.reduce_mean(image) + 0.5\n",
    "    image = tf.image.adjust_gamma(image, gamma=gamma)\n",
    "    ###########################################################################\n",
    "\n",
    "    # these also help training, but the gamma worked a little better, can play around with both\n",
    "    #if tf.random.uniform(()) > 0.50:\n",
    "    #    image = tf.image.adjust_brightness(image, delta=0.2)\n",
    "\n",
    "    #if tf.random.uniform(()) > 0.50:\n",
    "    #    image = tf.image.adjust_contrast(image, contrast_factor=0.2)\n",
    "\n",
    "    image = tf.clip_by_value(image, 0., 1.)  # after majority of augmentations, clip back to 0, 1 before returning\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def load_image_mask(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    # load image from file name\n",
    "    file_path = parms.TRAIN_PATH + \"/images/\" + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    # convert the png compressed string to a 3D tensor\n",
    "    image = tf.image.decode_png(image, channels=parms.IMAGE_CHANNELS)\n",
    "    image = tf.image.convert_image_dtype(image, parms.IMAGE_DTYPE)\n",
    "\n",
    "    # load the mask\n",
    "    file_path = parms.TRAIN_PATH + \"/masks/\" + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    mask_orig = tf.io.read_file(file_path)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    mask_orig = tf.image.decode_png(mask_orig, channels=1)\n",
    "    # Convert to 1 or 0\n",
    "    mask_adj = tf.where(mask_orig > 0, 1, 0)\n",
    "    mask_adj = tf.cast(mask_adj, dtype=tf.float32)\n",
    "\n",
    "    # actual mask - used for model verification, ignored for training\n",
    "    mask_orig = tf.where(mask_orig > 0, 1, 0)\n",
    "    mask_orig = tf.cast(mask_orig, dtype=tf.float32)\n",
    "\n",
    "    return image, mask_adj, mask_orig\n",
    "\n",
    "def image_mask_resize(image, mask):\n",
    "    # resize image and mask\n",
    "    image = tf.image.resize(image, [parms.IMAGE_ROWS, parms.IMAGE_COLS]) \n",
    "    mask = tf.image.resize(mask, [parms.IMAGE_ROWS, parms.IMAGE_COLS]) \n",
    "    return image, mask\n",
    "\n",
    "# mapped method from training to load image and mask\n",
    "def process_train_image_id(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    image, mask_adj, mask_orig = load_image_mask(image_id)\n",
    "    image, mask_adj = image_mask_aug(image, mask_adj)\n",
    "\n",
    "    # You can resize then augment, or augment then resize\n",
    "    # Since this enlarges the image, I like to do the augmentation first, then resize\n",
    "    image, mask_adj = image_mask_resize(image, mask_adj)\n",
    "\n",
    "    return image, mask_adj\n",
    "\n",
    "# mapped method for validation to load image and mask\n",
    "def process_val_image_id(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    image, mask_adj, mask_orig = load_image_mask(image_id)\n",
    "\n",
    "    # did not apply gamma just to see if any difference...\n",
    "    #gamma = tf.math.reduce_mean(image) + 0.5\n",
    "    #image = tf.image.adjust_gamma(image, gamma=gamma)\n",
    "    ###########################################################################\n",
    "\n",
    "    image, mask_adj = image_mask_resize(image, mask_adj)\n",
    "\n",
    "    return image, mask_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDP15hLoHTTL"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in train_dataset.take(2):\n",
    "    print(\"Image id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# map training images to processing, includes any augmentation\n",
    "train_dataset = train_dataset.map(process_train_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, mask in train_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Encoded Pixels shape: \", mask.numpy().shape)\n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask.numpy()\n",
    "\n",
    "#show_batch_mask([some_image, some_mask])\n",
    "\n",
    "train_dataset = train_dataset.batch(parms.BATCH_SIZE).repeat()\n",
    "#train_dataset = train_dataset.cache().batch(parms.BATCH_SIZE).repeat()\n",
    "\n",
    "# Show the images, execute this cell multiple times to see the images\n",
    "for image, mask in train_dataset.take(1):\n",
    "    sample_image, sample_mask = image[0], mask[0]\n",
    "show_batch_mask([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c843I8e9HTTR"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pd\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(valid_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in val_dataset.take(2):\n",
    "    print(\"Image id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "val_dataset = val_dataset.map(process_val_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, mask in val_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Encoded Pixels shape: \", mask.numpy().shape)\n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask.numpy()\n",
    "\n",
    "#show_batch_mask([some_image, some_mask])\n",
    "#val_dataset = val_dataset.cache().batch(parms.BATCH_SIZE).repeat()  # uncomment if there is enough memory, speeds up training\n",
    "\n",
    "val_dataset = val_dataset.cache().batch(parms.BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGN_JOpAHTTT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final check before model training.  I added a string of the mask non-zero counts - need to make sure the masks \n",
    "# were created ok.  (got bit by this one after a small change....)\n",
    "\n",
    "# Test Validation or Train by changing the dataset\n",
    "\n",
    "mask_cnt_str = \"\"\n",
    "sample_image = None\n",
    "sample_mask = None\n",
    "for image, mask in val_dataset.take(1):\n",
    "    image_np = image.numpy()\n",
    "    mask_np = mask.numpy()\n",
    "    for i in range(len(image_np)):\n",
    "        #show_batch_mask([image[i], mask[i]])  # Will show all of the images in the batch\n",
    "        mask_cnt_str = mask_cnt_str + str(np.count_nonzero(mask_np[i])) + \"  \"\n",
    "        #print(\"Mask shape: {}  Max: {}  Min: {}\".format(mask.numpy().shape, np.max(mask.numpy()), np.min(mask.numpy())))\n",
    "\n",
    "        if np.count_nonzero(mask_np[i]) > 0:\n",
    "            sample_image, sample_mask = image[i], mask[i]\n",
    "            \n",
    "print(\"Mask counts: \", mask_cnt_str)\n",
    "show_batch_mask([sample_image, sample_mask])  # Will show the sample masks, if errors, then no mask was found content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_L88A2M8HTTZ"
   },
   "outputs": [],
   "source": [
    "# If you want to see the improvements after each EPOC, add to the callback. Helps to make sure show_predictions works...\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "# Normal callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_lb_metric', factor=0.33, patience=4, verbose=1, mode='max', min_delta=0.0001, \n",
    "                              cooldown=0, min_lr=1e-8)\n",
    "earlystopper = EarlyStopping(monitor=\"val_lb_metric\", mode=\"max\", verbose=2, patience=10)\n",
    "checkpointer = ModelCheckpoint(parms.MODEL_PATH, monitor='val_lb_metric', verbose=1, mode=\"max\", save_best_only=True)\n",
    " \n",
    "# Methods to support training verification\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = np.where(pred_mask > 0.5, 1, 0)\n",
    "    return pred_mask[0]\n",
    "\n",
    "# Shows the image, original mask and predicted mask\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            show_batch_mask([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        show_batch_mask([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "# Taken from https://github.com/ybabakhin/kaggle_salt_bes_phalanx\n",
    "loss_function = \"bce_dice\"  # bce_dice, lovasz\n",
    "\n",
    "def loss(y, p):\n",
    "    return dice_coef_loss_bce(y, p, dice=0.5, bce=0.5)\n",
    "\n",
    "def lb_metric(y_true, y_pred):\n",
    "    return Kaggle_IoU_Precision(y_true, y_pred, threshold=0 if loss_function == 'lovasz' else 0.5)\n",
    "    \n",
    "def compile_model_unet(parms, model):\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "                loss= make_loss(loss_function),\n",
    "                metrics=[lb_metric])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUcdJBvMHTTc"
   },
   "outputs": [],
   "source": [
    "#UNet Builder from Yus Has: https://www.kaggle.com/yushas/imageprocessingtips\n",
    "# I use this whenever I need a u-net, very easy to shrink or grow size/levels as needed.\n",
    "# His variables are a bit cryptic, but it is his code, so I left them as is....\n",
    "\n",
    "def conv_block_mod(m, dim, acti, bn, res, do=0):\n",
    "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
    "    n = tf.keras.layers.Dropout(do)(n) if do else n\n",
    "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
    "    return tf.keras.layers.Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block_mod(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block_mod(m, dim, acti, bn, res)\n",
    "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block_mod(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = tf.keras.layers.UpSampling2D()(m)\n",
    "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = tf.keras.layers.Concatenate()([n, m])\n",
    "        m = conv_block_mod(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block_mod(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "def UNet_mod(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=False, batchnorm=True, maxpool=True, upconv=True, residual=False):\n",
    "    i = tf.keras.layers.Input(shape=img_shape)\n",
    "#    s = Lambda(lambda x: x / 255) (i)\n",
    "    o = level_block_mod(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)#Unet\n",
    "    o = tf.keras.layers.Conv2D(out_ch, 1, activation=parms.FINAL_ACTIVATION)(o)\n",
    "    return tf.keras.Model(inputs=i, outputs=o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yllVlpLHTTf"
   },
   "outputs": [],
   "source": [
    "# Build the model, comment if loading prior trained model\n",
    "\n",
    "model=UNet_mod(parms.IMAGE_DIM,\n",
    "               out_ch=parms.NUM_CLASSES,\n",
    "               start_ch=32,\n",
    "               depth=5,\n",
    "               batchnorm=True,\n",
    "               dropout=0.5)\n",
    "model = compile_model_unet(parms, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO4KYpTGdKyA"
   },
   "outputs": [],
   "source": [
    "# uncomment to draw the model...\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J17CIrrAHTTi"
   },
   "outputs": [],
   "source": [
    "# Uncomment to reload the model from prior runs\n",
    "#model = load_model(parms.MODEL_PATH, custom_objects={'loss': loss, 'lb_metric': lb_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2v2G9ofeHTTl"
   },
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=parms.EPOCS, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[reduce_lr, earlystopper, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qWuiOTkHTTn"
   },
   "outputs": [],
   "source": [
    "# graph training...\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.figure()\n",
    "history_df[['loss', 'val_loss']].plot(title=\"Loss\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Loss')\n",
    "history_df[['lb_metric', 'val_lb_metric']].plot(title=\"Kaggle LB\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05hfburuHTTt"
   },
   "source": [
    "# Validate the training...\n",
    "\n",
    "- Show images and predictions\n",
    "- Try and improve the prediction by cleaning up the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOA8VvtawUtj"
   },
   "outputs": [],
   "source": [
    "# Reload the best model from prior run\n",
    "model = load_model(parms.MODEL_PATH, custom_objects={'loss': loss, 'lb_metric': lb_metric})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smuMAXCqHTTu"
   },
   "outputs": [],
   "source": [
    "# Easy to modify to predict the given test files that do not have a mask\n",
    "\n",
    "# Method to be applied to all testing images\n",
    "def process_test_image_id(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    image, mask_adj, mask_orig = load_image_mask(image_id)  \n",
    "\n",
    "    # since applied to all iamges, apply for testing validation\n",
    "    gamma = tf.math.reduce_mean(image) + 0.5\n",
    "    image = tf.image.adjust_gamma(image, gamma=gamma)\n",
    "    ###########################################################\n",
    "    \n",
    "    image, mask_adj = image_mask_resize(image, mask_adj)\n",
    "\n",
    "    return image_id, image, mask_adj, mask_orig\n",
    "\n",
    "# Create Dataset from pd\n",
    "test_df = shuffle(valid_df) # take out if you want the same every time...\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in test_dataset.take(2):\n",
    "    print(image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "test_dataset = test_dataset.map(process_test_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image_id, image, mask_adj, mask_orig in test_dataset.take(1):\n",
    "    print(\"Image Id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"(128, 128) mask shape: \", mask_adj.numpy().shape)\n",
    "    print(\"(101, 101) mask shape: \", mask_orig.numpy().shape)    \n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask_adj.numpy()\n",
    "\n",
    "test_dataset = test_dataset.batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9NrQod3K5tX"
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, dilation, disk, flood_fill\n",
    "\n",
    "def mask_erosion_dilation(mask, disksize=6):\n",
    "    \"\"\"\n",
    "    # ONLY GRAYSCALE\n",
    "    erode and dialate an image - cleans up random pixels\n",
    "    opening operation. The purpose of this operation is to remove small\n",
    "    islands of noise while (trying to) maintain the areas of the larger\n",
    "    objects in your image\n",
    "    \"\"\"\n",
    "    #print(\"ero \", mask.shape)\n",
    "    mask = mask.reshape((mask.shape[0], mask.shape[1]))\n",
    "    selem = disk(disksize)\n",
    "    mask = dilation(erosion(mask, selem), selem)\n",
    "    return mask.reshape((mask.shape[0], mask.shape[1], 1))\n",
    "\n",
    "def show_batch_mask_aug(image_id, display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask', \"Aug Mask\", \"Adj to 101\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        #print(np.max(display_list[i]), np.min(display_list[i]))\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        if i == 0:\n",
    "            plt.title(image_id)\n",
    "        else:\n",
    "            plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]),cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def test_model_with_aug(dataset, model, steps, test=False):\n",
    "    if test and steps > 30:\n",
    "        print(\"CHANGE STEPS, TOO LARGE.... \", steps)\n",
    "        return\n",
    "\n",
    "    kaggle_percision_adj = 0.0\n",
    "    kaggle_percision_orig = 0.0\n",
    "    for image_id, image, mask_adj, mask_orig in tqdm(dataset.take(steps)):\n",
    "\n",
    "        pred_mask = model.predict(image)\n",
    "        mask_actual = create_mask(pred_mask)\n",
    "         \n",
    "        mask_aug = mask_erosion_dilation(mask_actual)\n",
    "        \n",
    "        # Score based on (128, 128) size\n",
    "        kaggle_percision_adj_tmp = Kaggle_IoU_Precision(mask_adj, np.expand_dims(mask_aug, axis=0)).numpy()\n",
    "        kaggle_percision_adj += kaggle_percision_adj_tmp\n",
    "\n",
    "        # Score based on (101, 101) size - Double check score after reducing\n",
    "        mask_aug_adj = tf.image.resize(mask_aug, [101, 101])\n",
    "        kaggle_percision_orig_tmp = Kaggle_IoU_Precision(mask_orig, np.expand_dims(mask_aug_adj, axis=0)).numpy()\n",
    "        kaggle_percision_orig += kaggle_percision_orig_tmp\n",
    "\n",
    "        if test:\n",
    "            img_id = image_id.numpy()[0].decode(\"utf-8\")\n",
    "            print(\"true vs true: \", Kaggle_IoU_Precision(mask_orig, mask_orig).numpy())\n",
    "            print(\"true vs pred: \", Kaggle_IoU_Precision(mask_adj, np.expand_dims(mask_actual, axis=0)).numpy())\n",
    "            print(\"true vs aug: \", Kaggle_IoU_Precision(mask_adj, np.expand_dims(mask_aug, axis=0)).numpy())\n",
    "            print(\"101 true vs 101 aug: \", Kaggle_IoU_Precision(mask_orig, np.expand_dims(mask_aug_adj, axis=0)).numpy())\n",
    "            print(\"non-zero counts,  act: \", np.count_nonzero(mask_adj), \"  pred: \", np.count_nonzero(mask_actual), \"  aug: \", np.count_nonzero(mask_aug))\n",
    "            print(\"101 non-zero counts,  act: \", np.count_nonzero(mask_orig), \"  pred: \", np.count_nonzero(mask_aug_adj))\n",
    "            show_batch_mask_aug(img_id, [image[0], mask_adj[0], mask_actual, mask_aug, mask_aug_adj])\n",
    "\n",
    "    kaggle_percision_adj = kaggle_percision_adj / steps\n",
    "    kaggle_percision_orig = kaggle_percision_orig / steps\n",
    "    return kaggle_percision_adj, kaggle_percision_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdpcnUiviM9k",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kaggle_percision_adj, kaggle_percision_orig = test_model_with_aug(test_dataset, model, 1395, test=False) # 5579 training  1395 validation\n",
    "print(\" \")\n",
    "print(\"(128, 128) Score: \", kaggle_percision_adj, \"   (101, 101) Score: \", kaggle_percision_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRbdy9gIltzK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Salt_Create_model_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
