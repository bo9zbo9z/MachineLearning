{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5slcdn4iWSTq"
   },
   "source": [
    "# Kaggle Ship Segmentation - Create Model on Kaggle Environment\n",
    "\n",
    "Link to competition: https://www.kaggle.com/c/airbus-ship-detection\n",
    "\n",
    "This notebook was converted from my prior Kaggle notebook.  Migrated to TF 2.x and converted various methods to be more native TF.  This will create and save a trained model.  The model is built from scratch, not pre-trained.  I do have links to a pre-trained option, but it did not perform as well as starting from scratch.  (Pre-trained is smaller, so had a harder time learning features.)\n",
    "\n",
    "The image size is (224, 224, 3), you can change it in the Config settings.  If you want to get the best possible result, the size should be increased as some features are small and when downsized lose detail.\n",
    "\n",
    "I stopped training after 7 epocs, val was over 75%, but still slowly learning.\n",
    "\n",
    "This notebook was about applying learning to use TensorFlow 2.x and Datasets, not to create a final model for submission.  There is much more fine-tuning todo to obtain 80% plus scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tn_PWaH1WSTu"
   },
   "outputs": [],
   "source": [
    "# Change to True if using Kaggle environment....\n",
    "USING_KAGGLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3zIITg_WST0"
   },
   "outputs": [],
   "source": [
    "# Normal includes...\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, warnings, time, copy, csv\n",
    "import numpy as np \n",
    "\n",
    "import IPython.display as display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "# This allows the runtime to decide how best to optimize CPU/GPU usage\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPUpxkxkPzGS"
   },
   "outputs": [],
   "source": [
    "# Config class that wraps global variable access, using personal libs is a pain in Kaggle, so copied in the class\n",
    "# Not all of the global vars are used, easier to jsut copy class over from lib\n",
    "\n",
    "class GlobalParms(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.keys_and_defaults = {\n",
    "         \"MODEL_NAME\": \"\",  # if you leave .h5 off, puts into a subdirectory\n",
    "         \"ROOT_PATH\": \"\",  # Location of the data for storing any data or files\n",
    "         \"TRAIN_DIR\": \"\",  # Subdirectory in the Root for Training files\n",
    "         \"TEST_DIR\": \"\",  # Optional subdirectory in  Root for Testing file\n",
    "         \"SUBMISSION_PATH\": None,  # Optional subdirectory for Contest files\n",
    "         \"MODEL_PATH\": None,  # Optional, subdirectory for saving/loading model\n",
    "         \"TRAIN_PATH\": None,  # Subdirectory in the Root for Training files\n",
    "         \"TEST_PATH\": None,  # Optional subdirectory in  Root for Testing file\n",
    "         \"SMALL_RUN\": False,   # Optional, run size will be reduced\n",
    "         \"NUM_CLASSES\": 0,  # Number of classes\n",
    "         \"CLASS_NAMES\": [],  # list of class names\n",
    "         \"IMAGE_ROWS\": 0,  # Row size of the image\n",
    "         \"IMAGE_COLS\": 0,  # Col size of the image\n",
    "         \"IMAGE_CHANNELS\": 0,  # Num of Channels, 1 for Greyscale, 3 for color\n",
    "         \"BATCH_SIZE\": 0,  # Number of images in each batch\n",
    "         \"EPOCS\": 0,  # Max number of training EPOCS\n",
    "         \"ROW_SCALE_FACTOR\": 1,  # Optional, allows scaling of an image.\n",
    "         \"COL_SCALE_FACTOR\": 1,  # Optional, allows scaling of an image.\n",
    "         \"IMAGE_EXT\": \".jpg\",  # Extent of the image file_ext\n",
    "         # Optional, default is np.float64, reduce memory by using np.float32\n",
    "         # or np.float16\n",
    "         \"IMAGE_DTYPE\": np.float32,\n",
    "         # Optional, change default if needed, can save memory space\n",
    "         \"Y_DTYPE\": np.int,\n",
    "         \"LOAD_MODEL\": False,  # Optional, If you want to load a saved model\n",
    "         \"SUBMISSION\": \"submission.csv\",  # Optional, Mainly used for Kaggle\n",
    "         \"METRICS\": ['accuracy'],  # ['categorical_accuracy'], ['accuracy']\n",
    "         \"FINAL_ACTIVATION\": 'sigmoid',  # sigmoid, softmax\n",
    "         \"LOSS\": \"\"  # 'binary_crossentropy', 'categorical_crossentropy'\n",
    "        }\n",
    "\n",
    "        self.__dict__.update(self.keys_and_defaults)\n",
    "        self.__dict__.update((k, v) for k, v in kwargs.items()\n",
    "                             if k in self.keys_and_defaults)\n",
    "\n",
    "        # Automatically reduce the training parms, change as needed\n",
    "        if self.__dict__[\"SMALL_RUN\"]:\n",
    "            self.__dict__[\"BATCH_SIZE\"] = 1\n",
    "            self.__dict__[\"EPOCS\"] = 2\n",
    "            self.__dict__[\"ROW_SCALE_FACTOR\"] = 1\n",
    "            self.__dict__[\"COL_SCALE_FACTOR\"] = 1\n",
    "\n",
    "        # Use configuration items to create real ones\n",
    "        self.__dict__[\"SCALED_ROW_DIM\"] = \\\n",
    "            np.int(self.__dict__[\"IMAGE_ROWS\"] /\n",
    "                   self.__dict__[\"ROW_SCALE_FACTOR\"])\n",
    "\n",
    "        self.__dict__[\"SCALED_COL_DIM\"] =  \\\n",
    "            np.int(self.__dict__[\"IMAGE_COLS\"] /\n",
    "                   self.__dict__[\"COL_SCALE_FACTOR\"])\n",
    "\n",
    "        if self.__dict__[\"TRAIN_PATH\"] is None:  # Not passed, so set it\n",
    "            self.__dict__[\"TRAIN_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"ROOT_PATH\"],\n",
    "                             self.__dict__[\"TRAIN_DIR\"])\n",
    "\n",
    "        if self.__dict__[\"TEST_PATH\"] is None:  # Not passed, so set it\n",
    "            self.__dict__[\"TEST_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"ROOT_PATH\"],\n",
    "                             self.__dict__[\"TEST_DIR\"])\n",
    "\n",
    "        if self.__dict__[\"SUBMISSION_PATH\"] is None:  # Not passed, so set\n",
    "            self.__dict__[\"SUBMISSION_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"ROOT_PATH\"],\n",
    "                             self.__dict__[\"SUBMISSION\"])\n",
    "        else:\n",
    "            self.__dict__[\"SUBMISSION_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"SUBMISSION_PATH\"],\n",
    "                             self.__dict__[\"SUBMISSION\"])\n",
    "\n",
    "        if self.__dict__[\"MODEL_PATH\"] is None:  # Not passed, so set it\n",
    "            self.__dict__[\"MODEL_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"ROOT_PATH\"],\n",
    "                             self.__dict__[\"MODEL_NAME\"])\n",
    "        else:\n",
    "            self.__dict__[\"MODEL_PATH\"] = \\\n",
    "                os.path.join(self.__dict__[\"MODEL_PATH\"],\n",
    "                             self.__dict__[\"MODEL_NAME\"])\n",
    "\n",
    "        self.__dict__[\"IMAGE_DIM\"] = \\\n",
    "            (self.__dict__[\"SCALED_ROW_DIM\"],\n",
    "             self.__dict__[\"SCALED_COL_DIM\"],\n",
    "             self.__dict__[\"IMAGE_CHANNELS\"])\n",
    "\n",
    "        if self.__dict__[\"IMAGE_CHANNELS\"] == 1:\n",
    "            self.__dict__[\"COLOR_MODE\"] = \"grayscale\"\n",
    "        else:\n",
    "            self.__dict__[\"COLOR_MODE\"] = \"rgb\"\n",
    "\n",
    "    def set_train_path(self, train_path):\n",
    "        self.__dict__[\"TRAIN_PATH\"] = train_path\n",
    "\n",
    "    def set_class_names(self, class_name_list):\n",
    "        self.__dict__[\"CLASS_NAMES\"] = class_name_list\n",
    "\n",
    "        if self.__dict__[\"NUM_CLASSES\"] != \\\n",
    "           len(self.__dict__[\"CLASS_NAMES\"]):\n",
    "            raise ValueError(\"ERROR number of classses do not match, Classes: \"\n",
    "                             + str(self.__dict__[\"NUM_CLASSES\"])\n",
    "                             + \" Class List: \"\n",
    "                             + str(self.__dict__[\"CLASS_NAMES\"]))\n",
    "\n",
    "    def print_contents(self):\n",
    "        print(self.__dict__)\n",
    "\n",
    "    def print_key_value(self):\n",
    "        for key, value in self.__dict__.items():\n",
    "            print(key, \":\", value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMK-FCdiWST_"
   },
   "source": [
    "## Various Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFpBD1NUWSUB"
   },
   "outputs": [],
   "source": [
    "# Set these to match your environment\n",
    "\n",
    "if USING_KAGGLE:\n",
    "    ROOT_PATH = \"../input/airbus-ship-detection/\"\n",
    "else:\n",
    "    ROOT_PATH = \"/Users/john/Documents/ImageData/KaggleShip/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "        \n",
    "# Establish global dictionary\n",
    "parms = GlobalParms(ROOT_PATH=ROOT_PATH,\n",
    "                    MODEL_NAME=\"airModel.h5\",\n",
    "                    MODEL_PATH=\"\"\n",
    "                    TRAIN_DIR=\"train_v2\", \n",
    "                    NUM_CLASSES=1,\n",
    "                    IMAGE_ROWS=224,\n",
    "                    IMAGE_COLS=224,\n",
    "                    IMAGE_CHANNELS=3,\n",
    "                    BATCH_SIZE=16,\n",
    "                    EPOCS=8,\n",
    "                    FINAL_ACTIVATION=\"sigmoid\",\n",
    "                    IMAGE_EXT=\".jpg\")\n",
    "\n",
    "parms.print_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWpSlnmVWSUG"
   },
   "outputs": [],
   "source": [
    "# Encodes a mask, only used to verify training\n",
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    \n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VT2xKt1nWSUL"
   },
   "outputs": [],
   "source": [
    "def show_batch_mask(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES3pwdKpWSUR"
   },
   "source": [
    "## Create training and validation files\n",
    "\n",
    "The number of ships is not balanced and the size of some of the images are very small.  The approach can be changed if you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "colab": {},
    "colab_type": "code",
    "id": "YLfIJShCWSUT"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hmendonca/u-net-model-with-submission\n",
    "all_df = pd.read_csv(os.path.join(parms.ROOT_PATH, \"train_ship_segmentations_v2.csv\"))\n",
    "not_empty = pd.notna(all_df.EncodedPixels)\n",
    "print(not_empty.sum(), 'masks in', all_df[not_empty].ImageId.nunique(), 'images')\n",
    "print((~not_empty).sum(), 'empty images in', all_df.ImageId.nunique(), 'total images')\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSgWYkcYWSUZ"
   },
   "outputs": [],
   "source": [
    "# Add columns and built a unique list of image_ids.\n",
    "\n",
    "all_df['ships'] = all_df['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = all_df.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "# some files are too small/corrupt\n",
    "unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                               os.stat(os.path.join(parms.TRAIN_PATH, \n",
    "                                                                                    c_img_id)).st_size/1024)\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb'] > 50] # keep only +50kb files\n",
    "unique_img_ids['file_size_kb'].hist()\n",
    "all_df.drop(['ships'], axis=1, inplace=True)\n",
    "unique_img_ids.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rK6dMVSWSUe"
   },
   "outputs": [],
   "source": [
    "# Shows the unblanced, most have no ships, so need to change training set to have more with ships and \n",
    "# less without ships\n",
    "unique_img_ids['ships'].hist(bins=unique_img_ids['ships'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLTB1uWrWSUj"
   },
   "outputs": [],
   "source": [
    "# Bqlance rows\n",
    "SAMPLES_PER_GROUP = 1500\n",
    "balanced_train_df = unique_img_ids.groupby('ships').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "balanced_train_df['ships'].hist(bins=balanced_train_df['ships'].max()+1)\n",
    "print(balanced_train_df.shape[0], 'masks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEqMBiMRWSUo"
   },
   "outputs": [],
   "source": [
    "# Create training and validation lists from the balanced df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_ids, valid_ids = train_test_split(balanced_train_df, \n",
    "                 test_size = 0.2, \n",
    "                 stratify = balanced_train_df['ships'])  #Try and make sure nice distribution between train and val\n",
    "train_df = pd.merge(all_df, train_ids)\n",
    "valid_df = pd.merge(all_df, valid_ids)\n",
    "train_df = shuffle(train_df) # Shuffle since same image would be grouped\n",
    "print(train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_S8I3ViWSUt"
   },
   "outputs": [],
   "source": [
    "# Can double check....\n",
    "#valid_df['ships'].hist(bins=train_df['ships'].max()+1)\n",
    "#train_df['ships'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8e8VIlUFWSUw"
   },
   "outputs": [],
   "source": [
    "# set lengths and steps\n",
    "train_len = len(train_df)\n",
    "val_len = len(valid_df)\n",
    "images_list_len = train_len + val_len\n",
    "\n",
    "steps_per_epoch = np.ceil(train_len // parms.BATCH_SIZE) # set step sizes based on train & batch\n",
    "validation_steps = np.ceil(val_len // parms.BATCH_SIZE) # set step sizes based on val & batch\n",
    "\n",
    "print(\"Total number: \", images_list_len, \"  Train number: \", train_len, \"  Val number: \", val_len)\n",
    "print(\"Steps/EPOC: \", steps_per_epoch, \"  Steps/Validation: \", validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj8gJz3AWSU1"
   },
   "source": [
    "## Build, load and augment TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wo5SrgekWSU2"
   },
   "outputs": [],
   "source": [
    "# Decode the image, convert to float, normalize by 255 and resize\n",
    "def decode_img(image: tf.Tensor) -> tf.Tensor:\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=parms.IMAGE_CHANNELS)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    image = tf.image.convert_image_dtype(image, parms.IMAGE_DTYPE)\n",
    "    return image\n",
    "\n",
    "def resize_image_mask(image: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.image.resize(image, [parms.IMAGE_ROWS, parms.IMAGE_COLS])\n",
    "    mask = tf.image.resize(mask, [parms.IMAGE_ROWS, parms.IMAGE_COLS])\n",
    "    return image, mask\n",
    "\n",
    "def image_mask_aug(image, mask):\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:    \n",
    "        k = tf.random.uniform(shape=[], minval=1, maxval=3, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k) #0-4, 0/360, 90/180/270\n",
    "        mask = tf.image.rot90(mask, k) #0-4, 0/360, 90/180/270\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "        \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        mask = tf.image.flip_up_down(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def mask_wrapper(image_id_in):\n",
    "    image_id = image_id_in.numpy().decode(\"utf-8\")\n",
    "    #print(type(image_id), image_id)\n",
    "    encoded_pixels = all_df.query('ImageId==@image_id')['EncodedPixels']\n",
    "    #print(\"EP \", encoded_pixels)\n",
    "    mask = masks_as_image(encoded_pixels)\n",
    "    return tf.convert_to_tensor(mask, dtype=tf.int16)\n",
    "\n",
    "\n",
    "# method mapped to load image and mask\n",
    "def process_train_image_id(image_id: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "    [mask,] = tf.py_function(mask_wrapper, [image_id], [tf.int16])  #parms must be tensors\n",
    "    mask.set_shape((768,768, 1))\n",
    "\n",
    "    file_path = parms.TRAIN_PATH + \"/\" + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = decode_img(image)\n",
    "    \n",
    "    image, mask = resize_image_mask(image, mask)\n",
    "    image, mask = image_mask_aug(image, mask)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "def process_val_image_id(image_id: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "    [mask,] = tf.py_function(mask_wrapper, [image_id], [tf.int16])  #parms must be tensors\n",
    "    mask.set_shape((768,768, 1))\n",
    "\n",
    "    file_path = parms.TRAIN_PATH + \"/\" + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = decode_img(image)\n",
    "    \n",
    "    image, mask = resize_image_mask(image, mask)\n",
    "       \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yj6RWCqJWSU8"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in train_dataset.take(2):\n",
    "    print(\"Image id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# map training images to processing, includes any augmentation\n",
    "train_dataset = train_dataset.map(process_train_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, mask in train_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Encoded Pixels shape: \", mask.numpy().shape)\n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask.numpy()\n",
    "\n",
    "#show_batch_mask([some_image, some_mask])\n",
    "\n",
    "train_dataset = train_dataset.batch(parms.BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-HBARzmWSVp"
   },
   "outputs": [],
   "source": [
    "# Show the images, execute this cell multiple times to see the images\n",
    "for image, mask in train_dataset.take(1):\n",
    "    sample_image, sample_mask = image[0], mask[0]\n",
    "show_batch_mask([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGNwIor4WSVt"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pd\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(valid_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in val_dataset.take(2):\n",
    "    print(\"Image id: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "val_dataset = val_dataset.map(process_val_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, mask in val_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Encoded Pixels shape: \", mask.numpy().shape)\n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask.numpy()\n",
    "\n",
    "#show_batch_mask([some_image, some_mask])\n",
    "\n",
    "val_dataset = val_dataset.batch(parms.BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2_PmP3CWSVw"
   },
   "outputs": [],
   "source": [
    "# Final check before model training.  I added a string of the mask non-zero counts - need to make sure the masks \n",
    "# were created ok.  (got bit by this one after a small change....)\n",
    "\n",
    "# Test Validation or Train by changing the dataset\n",
    "\n",
    "mask_cnt_str = \"\"\n",
    "sample_image = None\n",
    "sample_mask = None\n",
    "#for image, mask in train_dataset.take(1):\n",
    "for image, mask in val_dataset.take(1):\n",
    "    image_np = image.numpy()\n",
    "    mask_np = mask.numpy()\n",
    "    for i in range(len(image_np)):\n",
    "        #show_batch_mask([image[i], mask[i]])  # Will show all of the batch\n",
    "        mask_cnt_str = mask_cnt_str + str(np.count_nonzero(mask_np[i])) + \"  \"\n",
    "        #print(\"Mask shape: {}  Max: {}  Min: {}\".format(mask.numpy().shape, np.max(mask.numpy()), np.min(mask.numpy())))\n",
    "\n",
    "        if np.count_nonzero(mask_np[i]) > 0:\n",
    "            sample_image, sample_mask = image[i], mask[i]\n",
    "            \n",
    "print(\"Mask counts: \", mask_cnt_str)\n",
    "show_batch_mask([sample_image, sample_mask])  # Will show the sample masks, if errors, then no mask was found content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaBs1MOaWSVy"
   },
   "outputs": [],
   "source": [
    "# Pre-trained model from this article.  Could not build it on Kaggle, built it locally, then loaded as personal data set.\n",
    "# Did not train as well as building a new model.  Move cell down if you want to play with it....\n",
    "# The article is VERY good!!\n",
    "#https://www.tensorflow.org/tutorials/images/segmentation\n",
    "\n",
    "#https://tensorlayer.com\n",
    "#def iou_coe(output, target, threshold=0.5, axis=(1, 2, 3), smooth=1e-5):\n",
    "#    pre = tf.cast(output > threshold, dtype=tf.float32)\n",
    "#    truth = tf.cast(target > threshold, dtype=tf.float32)\n",
    "#    inse = tf.reduce_sum(tf.multiply(pre, truth), axis=axis)  # AND\n",
    "#    union = tf.reduce_sum(tf.cast(tf.add(pre, truth) >= 1, dtype=tf.float32), axis=axis)  # OR\n",
    "#    batch_iou = (inse + smooth) / (union + smooth)\n",
    "#    iou = tf.reduce_mean(batch_iou, name='iou_coe')\n",
    "#    return iou  # , pre, truth, inse, union\n",
    "\n",
    "#def compile_model_pre_trained(parms, model):\n",
    "#    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-6),\n",
    "#                loss=combo_loss,\n",
    "#                metrics=[iou_coe])\n",
    "#    return model\n",
    "\n",
    "#model = load_model(\"../input/unetmodel/baseModel.h5\")\n",
    "#model = compile_model_pre_trained(parms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzVfXGUrWSV1"
   },
   "outputs": [],
   "source": [
    "# If you want to see the improvements after each EPOC, add to the callback. Helps to make sure show_predictions works...\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "# Normal callbacks\n",
    "# I monitor val_loss, just seemed to work better....\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.33, patience=1, verbose=1, mode='min', min_delta=0.0001, \n",
    "                              cooldown=0, min_lr=1e-8)\n",
    "earlystopper = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=10)\n",
    "checkpointer = ModelCheckpoint(parms.MODEL_PATH, monitor='val_loss', verbose=1, mode=\"auto\", save_best_only=True)\n",
    " \n",
    "# Methods to support training verification\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = np.where(pred_mask > 0.5, 1, 0)\n",
    "    return pred_mask[0]\n",
    "\n",
    "# Shows the image, original mask and predicted mask\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            show_batch_mask([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        show_batch_mask([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "       \n",
    "# https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/\n",
    "def combo_loss(y_true, y_pred):\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
    "        denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n",
    "        return tf.reshape(1 - numerator / denominator, (-1, 1, 1))\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True) + dice_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def compile_model_unet(parms, model):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-6),\n",
    "                loss=combo_loss,\n",
    "                metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3q6fKKGGWSV4"
   },
   "outputs": [],
   "source": [
    "#UNet Builder: https://www.kaggle.com/yushas/imageprocessingtips\n",
    "# I use this whenever I need a u-net, very easy to shrink or grow size/levels as needed.\n",
    "\n",
    "def conv_block_mod(m, dim, acti, bn, res, do=0):\n",
    "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
    "    n = tf.keras.layers.Dropout(do)(n) if do else n\n",
    "    n = tf.keras.layers.Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = tf.keras.layers.BatchNormalization()(n) if bn else n\n",
    "    return tf.keras.layers.Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block_mod(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block_mod(m, dim, acti, bn, res)\n",
    "        m = tf.keras.layers.MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block_mod(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)#再帰\n",
    "        if up:\n",
    "            m = tf.keras.layers.UpSampling2D()(m)\n",
    "            m = tf.keras.layers.Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = tf.keras.layers.Concatenate()([n, m])\n",
    "        m = conv_block_mod(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block_mod(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "def UNet_mod(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=False, batchnorm=True, maxpool=True, upconv=True, residual=False):\n",
    "    i = tf.keras.layers.Input(shape=img_shape)\n",
    "#    s = Lambda(lambda x: x / 255) (i)\n",
    "    o = level_block_mod(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)#Unet\n",
    "    o = tf.keras.layers.Conv2D(out_ch, 1, activation=parms.FINAL_ACTIVATION)(o)\n",
    "    return tf.keras.Model(inputs=i, outputs=o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv_g7bWGWSV8"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model=UNet_mod(parms.IMAGE_DIM,\n",
    "               out_ch=parms.NUM_CLASSES,\n",
    "               start_ch=16,\n",
    "               depth=4,\n",
    "               batchnorm=True,\n",
    "               dropout=0.5)\n",
    "model = compile_model_unet(parms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RNwmgTSWSV_"
   },
   "outputs": [],
   "source": [
    "# Reload the model from prior runs\n",
    "#model = load_model(parms.MODEL_PATH, custom_objects={'combo_loss': combo_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AiBKnfWWSWC"
   },
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "# You need to download the saved model and/or move to your personal dataset\n",
    "# Once session ends, temp workspace is lost\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=parms.EPOCS, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[reduce_lr, earlystopper, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFoZt2xBWSWF"
   },
   "outputs": [],
   "source": [
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.figure()\n",
    "history_df[['loss', 'val_loss']].plot(title=\"Loss\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Loss')\n",
    "history_df[['mean_io_u', 'val_mean_io_u']].plot(title=\"Mean IOU\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Accuracy')\n",
    "#history_df[['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n",
    "#plt.xlabel('Epocs')\n",
    "#plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5bBr0MtWSWI"
   },
   "outputs": [],
   "source": [
    "#history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpP4qbg2WSWL"
   },
   "source": [
    "### Validate the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPgQbjuwWSWM"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pd\n",
    "test_df = shuffle(valid_df)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_df[\"ImageId\"].values)\n",
    "\n",
    "# Verify image paths were loaded\n",
    "for image_id in test_dataset.take(2):\n",
    "    print(image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "test_dataset = test_dataset.map(process_val_image_id, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, mask in test_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Encoded Pixels shape: \", mask.numpy().shape)\n",
    "    some_image = image.numpy()\n",
    "    some_mask = mask.numpy()\n",
    "\n",
    "#show_batch_mask([some_image, some_mask])\n",
    "\n",
    "test_dataset = test_dataset.batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ps11g7PWSWP"
   },
   "outputs": [],
   "source": [
    "show_predictions(test_dataset, 16)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ship_Create_model_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
