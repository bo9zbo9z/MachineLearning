{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl85J9Iqh4yQ"
   },
   "source": [
    "## Sparse - Kaggle Steel Defects - Segmentation (locate and identify defects)\n",
    "\n",
    "Link to competition: https://www.kaggle.com/c/severstal-steel-defect-detection\n",
    "\n",
    "This notebook was converted from prior Kaggle work.  Migrated to TF 2.x and converted various methods to be more native TF.  This was something I played with to see if using Sparse labels could help with the memory requirements and improve training.  It did help with memory, but training was similar to the other segmentation notebook.  I did not use this for the final merge of segmentation and classification.\n",
    "\n",
    "- Pre-trained model is from Pavel Yakubovshiy, (https://github.com/qubvel/segmentation_models) \n",
    "\n",
    "Final dice_coef score from Training images:  0.6055773677232597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xjJySeIXh_md",
    "outputId": "772a6231-8a30-4aac-ba02-5440ae080d34"
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# Google Collab specific stuff....\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "!ls \"/content/drive/My Drive\"\n",
    "\n",
    "USING_COLLAB = True\n",
    "%tensorflow_version 2.x\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "8kn1SgibviFV",
    "outputId": "379d0578-7ac3-49eb-b92d-83b44b22ace0"
   },
   "outputs": [],
   "source": [
    "# To start, install kaggle libs\n",
    "#!pip install -q kaggle\n",
    "\n",
    "# Workaround to install the newest version\n",
    "# https://stackoverflow.com/questions/58643979/google-colaboratory-use-kaggle-server-version-1-5-6-client-version-1-5-4-fai\n",
    "!pip install kaggle --upgrade --force-reinstall --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "xWXUL38Tvn8k",
    "outputId": "a3ee0a62-5443-45d4-f16c-61954baf061d"
   },
   "outputs": [],
   "source": [
    "# Upload your \"kaggle.json\" file that you created from your Kaggle Account tab\n",
    "# If you downloaded it, it would be in your \"Downloads\" directory\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZttY2gU-voIb",
    "outputId": "b908a0fc-9087-42aa-cbdb-a8130d773e60"
   },
   "outputs": [],
   "source": [
    "# On your VM, create kaggle directory and modify access rights\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aYXtZYElvoUy",
    "outputId": "afa21f71-afdf-43f2-f7f4-4ca12cdea905"
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions list\n",
    "!kaggle competitions download -c severstal-steel-defect-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tQSrMlPlwA_c",
    "outputId": "635b999f-8cde-4f69-ff45-57ef1083d0f3"
   },
   "outputs": [],
   "source": [
    "!unzip -uq severstal-steel-defect-detection.zip \n",
    "!ls train_images/a75bb4c01*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhuQ5I7Aevr7"
   },
   "outputs": [],
   "source": [
    "# Cleanup to add some space....\n",
    "!rm -r test_images\n",
    "!rm severstal-steel-defect-detection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6w1an-l5h4yT",
    "outputId": "d55420db-b087-4d81-f8de-d84eecbbffdf"
   },
   "outputs": [],
   "source": [
    "# Setup sys.path to find MachineLearning lib directory\n",
    "\n",
    "try: USING_COLLAB\n",
    "except NameError: USING_COLLAB = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if \"MachineLearning\" in sys.path[0]:\n",
    "    pass\n",
    "else:\n",
    "    print(sys.path)\n",
    "    if USING_COLLAB:\n",
    "        sys.path.insert(0, '/content/drive/My Drive/GitHub/MachineLearning/lib')  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    else:\n",
    "        sys.path.insert(0, '/Users/john/Documents/GitHub/MachineLearning/lib')  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    \n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1DL2FAhfsRf"
   },
   "outputs": [],
   "source": [
    "#%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "csTt9CUvh4yZ",
    "outputId": "f98b0d4c-3dcf-4b1a-cf3c-1039fe2ddccf"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, warnings, time, copy, csv, gc\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(\"AUTOTUNE: \", AUTOTUNE)\n",
    "\n",
    "from TrainingUtils import *\n",
    "from losses_and_metrics.Losses_Babakhin import make_loss, Kaggle_IoU_Precision, dice_coef_loss_bce\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", \"(Possibly )?corrupt EXIF data\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aplx71Xjh4yg"
   },
   "source": [
    "## Examine and understand data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PMdwqph-h4yd",
    "outputId": "6f8004e0-b4ff-457a-8943-53211fe079f5"
   },
   "outputs": [],
   "source": [
    "# GLOBALS/CONFIG ITEMS\n",
    "\n",
    "# Set root directory path to data\n",
    "if USING_COLLAB:\n",
    "    #ROOT_PATH = \"/content/drive/My Drive/ImageData/KaggleSteelDefects\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    ROOT_PATH = \"\"\n",
    "    MODEL_PATH= \"/content/drive/My Drive/ImageData/KaggleSteelDefects\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    \n",
    "else:\n",
    "    ROOT_PATH = \"/Users/john/Documents/ImageData/KaggleSteelDefects\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    MODEL_PATH= \"/Users/john/Documents/ImageData/KaggleSteelDefects\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "    \n",
    "# Establish global dictionary\n",
    "parms = GlobalParms(MODEL_NAME=\"model-SteelDefects-Sparse-Segmentation-V01.h5\",\n",
    "                    ROOT_PATH=ROOT_PATH,\n",
    "\n",
    "                    TRAIN_PATH=\"train_images\", \n",
    "                    MODEL_PATH=MODEL_PATH,\n",
    "                    SMALL_RUN=False,\n",
    "                    NUM_CLASSES=5,\n",
    "                    CLASS_NAMES=[\"Outside\", \"1\", \"2\", \"3\", \"4\"],\n",
    "                    IMAGE_ROWS=256,\n",
    "                    IMAGE_COLS=800,\n",
    "                    IMAGE_CHANNELS=3,\n",
    "                    BATCH_SIZE=16,\n",
    "                    EPOCS=20,\n",
    "                    IMAGE_EXT=\".jpg\",\n",
    "                    FINAL_ACTIVATION='sigmoid',\n",
    "                    LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "# Other globals...\n",
    "ORIG_MASK_SHAPE = (256, 1600)\n",
    "\n",
    "parms.print_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mhhvmqN7aFa"
   },
   "outputs": [],
   "source": [
    "# Simple helper method to display batches of images with labels....  \n",
    "\n",
    "def show_image_masks(image_in, masks_in):\n",
    "    if tf.is_tensor(image_in):\n",
    "        image = image_in.numpy()\n",
    "        masks = masks_in.numpy()\n",
    "    else:\n",
    "        image = image_in\n",
    "        masks = masks_in\n",
    "\n",
    "    #print(image.shape, masks.shape)\n",
    "\n",
    "    # cv2.polylines and cv2.findContours display better when range is 0-255\n",
    "    # https://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html\n",
    "    image = image * 255\n",
    "    palet = [(100, 100, 100), (249, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12)]\n",
    "    title = \"Labels: \"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "      \n",
    "    for j in range(1, parms.NUM_CLASSES):\n",
    "        #msk = np.ascontiguousarray(masks[:, :, j], dtype=np.uint8)\n",
    "        mask =  np.where(masks == j, 1, 0)\n",
    "        mask = mask.astype(np.uint8)\n",
    "        #print(mask.shape, mask.dtype)\n",
    "        if np.count_nonzero(mask) > 0:\n",
    "            title = title + str(j+1) + \",  \"\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "            for i in range(0, len(contours)):\n",
    "                cv2.polylines(image, contours[i], True, palet[j], 2) \n",
    "\n",
    "    title = title[:-3]  \n",
    "    ax.set_title(title)\n",
    "\n",
    "    #ax.imshow(tf.keras.preprocessing.image.array_to_img(image), cmap=plt.get_cmap('gray'))\n",
    "    #print(image.shape, image.dtype, np.max(image), np.min(image))\n",
    "    ax.imshow(image/255, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "def show_batch_image_masks(image, masks):\n",
    "    for i in range(len(image)):\n",
    "        show_image_masks(image[i], masks[i])\n",
    "\n",
    "# Helper methods to create mask's or rle's\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    \n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def build_masksORIG(rles, input_shape):\n",
    "    depth = len(rles)\n",
    "    masks = np.zeros((*input_shape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def build_masks2(rles, input_shape):\n",
    "    depth = len(rles)\n",
    "    masks = np.zeros((*input_shape,1))\n",
    "    #sparse_mask = np.zeros((*input_shape,1))\n",
    "    sparse_mask = np.zeros((parms.IMAGE_ROWS, parms.IMAGE_COLS, 1))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            masks = rle2mask(rle, input_shape)\n",
    "            masks = masks.reshape((*input_shape, 1))\n",
    "            masks = np.resize(masks, (parms.IMAGE_ROWS, parms.IMAGE_COLS, 1))\n",
    "\n",
    "            sparse_mask = np.where(masks > 0.5, i+1, sparse_mask)\n",
    "            #sparse_mask = np.where(masks == 1, i+1, sparse_mask)\n",
    "\n",
    "    return sparse_mask\n",
    "\n",
    "def build_masks(rles, input_shape):\n",
    "    depth = len(rles)\n",
    "    masks = np.zeros((*input_shape,1))\n",
    "    sparse_mask = np.zeros((*input_shape,1))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            masks = rle2mask(rle, input_shape)\n",
    "            masks = masks.reshape((*input_shape, 1))\n",
    "            #masks = np.resize(masks, (parms.IMAGE_ROWS, parms.IMAGE_COLS, 1))\n",
    "\n",
    "            #sparse_mask = np.where(masks > 0.5, i+1, sparse_mask)\n",
    "            sparse_mask = np.where(masks == 1, i+1, sparse_mask)\n",
    "\n",
    "    return sparse_mask\n",
    "\n",
    "def build_rles(masks):\n",
    "    width, height, depth = masks.shape\n",
    "    \n",
    "    rles = [mask2rle(masks[:, :, i])\n",
    "            for i in range(depth)]\n",
    "    \n",
    "    return rles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "VlqtDZyuNZwH",
    "outputId": "f55a66db-f7d7-4df1-fea1-f08bf492794b"
   },
   "outputs": [],
   "source": [
    "# Load train DEFECT csv\n",
    "image_defect_df = pd.read_csv(os.path.join(parms.ROOT_PATH, \"train.csv\"))\n",
    "\n",
    "# Load image file sizes for possible stratification usage\n",
    "image_defect_df['ImageSize'] = image_defect_df['ImageId'].map(lambda image_id: round(os.stat(os.path.join(parms.TRAIN_PATH, image_id)).st_size))\n",
    "#image_defect_df['ImageSize'] = 50\n",
    "\n",
    "print(image_defect_df.loc[image_defect_df[\"ImageId\"] == \"0025bde0c.jpg\"])\n",
    "image_defect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "cW1OhNAfnM9S",
    "outputId": "e76ed4b0-7d02-45e9-92b3-829447697ce0"
   },
   "outputs": [],
   "source": [
    "# Stratifing by image_size, my prior notebook used the number of white pixels, this was easier and gave a better spread\n",
    "image_defect_df_cut = pd.cut(image_defect_df[\"ImageSize\"], bins=[0, 85000, 104000, 115000, 1000000]) \n",
    "ax = image_defect_df_cut.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(20,6)) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3YRRc4omWOXS",
    "outputId": "38d9e9ac-1ed6-4234-a6c0-19ee21277d22"
   },
   "outputs": [],
   "source": [
    "# Apply method to create the group number\n",
    "def group_by_image_size(x):\n",
    "    #[0, 85000, 104000, 115000, 1000000])\n",
    "    if x < 85000:\n",
    "        return 0\n",
    "    elif x < 104000:\n",
    "        return 1\n",
    "    elif x < 115000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "image_defect_df['ImageGroup'] = image_defect_df['ImageSize'].apply(group_by_image_size)\n",
    "image_defect_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "YC21p-btRfhJ",
    "outputId": "00102aeb-8138-4112-b3d5-3ff48f4784c0"
   },
   "outputs": [],
   "source": [
    "# Select a balanced subset for training\n",
    "SAMPLES_PER_GROUP = 200000\n",
    "balanced_train_df = image_defect_df.groupby('ImageGroup').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "balanced_train_df['ImageGroup'].hist(bins=balanced_train_df['ImageGroup'].max()+1)\n",
    "print(balanced_train_df.shape[0], 'ImageGroup')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1R8KsrBkjgl"
   },
   "source": [
    "## Build an input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q2ncufD4SKG8",
    "outputId": "46125926-302a-4208-af9b-29d1b3c759ae"
   },
   "outputs": [],
   "source": [
    "# Split train and val, stratify by number of targets\n",
    "\n",
    "train_df, valid_df = train_test_split(balanced_train_df, \n",
    "                                      test_size = 0.2,\n",
    "                                      stratify = balanced_train_df['ImageGroup'])\n",
    "\n",
    "# Add some more training examples from the sparse examples\n",
    "#print('Original Training len: ', train_df.shape[0], \"  Validation len: \", valid_df.shape[0])\n",
    "#add_more_df = train_df.loc[train_df[\"DefectCount\"] > 1]\n",
    "#add_more_df = pd.concat([add_more_df, add_more_df])\n",
    "#train_df = pd.concat([train_df, add_more_df])\n",
    "#train_df.reset_index(drop=True)\n",
    "\n",
    "train_df = shuffle(train_df) # Shuffle\n",
    "\n",
    "print('After Adjust, Training len: ', train_df.shape[0], \"  Validation len: \", valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LAt1pTMSZtI8",
    "outputId": "9d4ff2d5-fda8-423e-9f82-133b05ee9468"
   },
   "outputs": [],
   "source": [
    "# set lengths and steps\n",
    "train_len = len(train_df)\n",
    "val_len = len(valid_df)\n",
    "images_list_len = train_len + val_len\n",
    "\n",
    "steps_per_epoch = np.ceil(train_len // parms.BATCH_SIZE) # set step sizes based on train & batch\n",
    "validation_steps = np.ceil(val_len // parms.BATCH_SIZE) # set step sizes based on val & batch\n",
    "\n",
    "print(\"Total number: \", images_list_len, \"  Train number: \", train_len, \"  Val number: \", val_len)\n",
    "print(\"Steps/EPOC: \", steps_per_epoch, \"  Steps/Validation: \", validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "AJ1h1bVeZ7CH",
    "outputId": "61c13ca2-0311-4301-a684-a45284b1d129"
   },
   "outputs": [],
   "source": [
    "# Final look at the distribution since we added more of the sparse cases\n",
    "print(train_df[\"ImageGroup\"].value_counts())\n",
    "print(valid_df[\"ImageGroup\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhwlsx-48daB"
   },
   "source": [
    "### Training and Validation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzIHTsEUcY2p"
   },
   "outputs": [],
   "source": [
    "# Read, decode the image, convert to float\n",
    "def read_decode_image(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    file_path = parms.TRAIN_PATH + \"/\" + image_id\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=parms.IMAGE_CHANNELS)\n",
    "    image = tf.image.convert_image_dtype(image, parms.IMAGE_DTYPE)\n",
    "    return image\n",
    "\n",
    "# Build mask(s) from rles\n",
    "def load_masks(image_id_in: tf.Tensor) -> tf.Tensor:\n",
    "    image_id = image_id_in.numpy().decode(\"utf-8\")\n",
    "    image_df = image_defect_df.loc[image_defect_df['ImageId'] == image_id]\n",
    "    #print(\"df \", image_id, image_df)\n",
    "\n",
    "    rles = [None] * parms.NUM_CLASSES # Create blank list\n",
    "    for i, image_row in image_df.iterrows():\n",
    "        indx = int(image_row[\"ClassId\"]) - 1\n",
    "        #print(\"row \", indx, image_row)\n",
    "        rles[indx] = image_row[\"EncodedPixels\"] # Fill in any encoded masks\n",
    "        \n",
    "    masks = build_masks(rles, input_shape=ORIG_MASK_SHAPE)\n",
    "\n",
    "    return masks\n",
    "\n",
    "# Augmentations for training dataset, done after cache\n",
    "def image_aug(image: tf.Tensor, masks: tf.Tensor) -> tf.Tensor:\n",
    "    # Must use custom precent, random.uniform, because both image and mask must match\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        masks = tf.image.flip_left_right(masks)\n",
    "        #for i in range(parms.NUM_CLASSES):\n",
    "        #    masks[:, :, i] = tf.image.flip_left_right(masks[:, :, i])\n",
    "        \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        masks = tf.image.flip_up_down(masks)\n",
    "        #for i in range(parms.NUM_CLASSES):\n",
    "        #    masks[:, :, i] = tf.image.flip_up_down(masks[:, :, i])\n",
    "\n",
    "    return image, masks\n",
    "\n",
    "# pre-cache mapped method to load image and masks\n",
    "def process_load_image_masks(image_id: tf.Tensor) -> tf.Tensor:\n",
    "    image = read_decode_image(image_id)  \n",
    "\n",
    "    [masks,] = tf.py_function(load_masks, [image_id], [tf.int32])  #parms must be tensors\n",
    "    #masks.set_shape((*ORIG_MASK_SHAPE, parms.NUM_CLASSES))\n",
    "    masks.set_shape((parms.IMAGE_ROWS, parms.IMAGE_COLS, 1))\n",
    "    #masks.set_shape((*ORIG_MASK_SHAPE, 1))\n",
    "    \n",
    "    image = tf.image.resize(image, [parms.IMAGE_ROWS, parms.IMAGE_COLS])\n",
    "    masks = tf.image.resize(masks, [parms.IMAGE_ROWS, parms.IMAGE_COLS])\n",
    "    return image, masks\n",
    "\n",
    "# post-cache mapped method, does image augmentation\n",
    "def process_train_post_cache(image: tf.Tensor, masks: tf.Tensor) -> tf.Tensor:\n",
    "    image, masks = image_aug(image, masks)\n",
    "    return image, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MxUIepA5rF-"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_df[\"ImageId\"].values)\n",
    "                                               \n",
    "# Verify image and label were loaded\n",
    "for image_id in train_dataset.take(2):\n",
    "    train_image_id = image_id.numpy().decode(\"utf-8\")\n",
    "    print(\"Image ID: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# map training images to processing, includes any augmentation\n",
    "train_dataset = train_dataset.map(process_load_image_masks, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, masks in train_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Masks shape: {}  Max: {}  Min: {}\".format(masks.numpy().shape, np.max(masks.numpy()), np.min(masks.numpy())))\n",
    "    some_image = image.numpy()\n",
    "    some_masks = masks.numpy()\n",
    "\n",
    "# Remove cache if running under Kaggle\n",
    "train_dataset = train_dataset.cache(\"./steel_train_seg2.tfcache\") \\\n",
    "#train_dataset = train_dataset \\\n",
    "                             .map(process_train_post_cache, num_parallel_calls=AUTOTUNE) \\\n",
    "                             .batch(parms.BATCH_SIZE) \\\n",
    "                             .prefetch(1) \\\n",
    "                             .repeat()\n",
    "\n",
    "# Uncomment to show the batch of images, execute this cell multiple times to see the images\n",
    "for batch_image, batch_masks in train_dataset.take(1):\n",
    "    show_batch_image_masks(batch_image, batch_masks)\n",
    "\n",
    "show_batch_image_masks([some_image], [some_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EF_khIPYe2-y"
   },
   "outputs": [],
   "source": [
    "# Double check that training labels and image_id are all good, can use different image_id's\n",
    "image_defect_df.loc[image_defect_df[\"ImageId\"] == train_image_id]\n",
    "#image_defect_df.loc[image_defect_df[\"ImageId\"] == \"3604dfc38.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h8z2RKofa9y"
   },
   "outputs": [],
   "source": [
    "# Create Dataset from pd\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(valid_df[\"ImageId\"].values)\n",
    "\n",
    "\n",
    "# Verify image and label were loaded\n",
    "for image_id in val_dataset.take(2):\n",
    "    val_image_id = image_id.numpy().decode(\"utf-8\")\n",
    "    print(\"Image ID: \", image_id.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    # map training images to processing, includes any augmentation\n",
    "val_dataset = val_dataset.map(process_load_image_masks, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Verify the mapping worked\n",
    "for image, masks in val_dataset.take(1):\n",
    "    print(\"Image shape: {}  Max: {}  Min: {}\".format(image.numpy().shape, np.max(image.numpy()), np.min(image.numpy())))\n",
    "    print(\"Masks shape: {}  Max: {}  Min: {}\".format(masks.numpy().shape, np.max(masks.numpy()), np.min(masks.numpy())))\n",
    "    some_image = image\n",
    "    some_masks = masks\n",
    "\n",
    "# Remove cache if running under Kaggle\n",
    "val_dataset = val_dataset.cache(\"./steel_val_seg2.tfcache2\") \\\n",
    "                         .batch(parms.BATCH_SIZE) \\\n",
    "                         .prefetch(1) \\\n",
    "                         .repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YipJyMNThFtB"
   },
   "outputs": [],
   "source": [
    "# Double check that val labels and image_id are all good, can use different image_id's\n",
    "image_defect_df.loc[image_defect_df[\"ImageId\"] == val_image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juyUqlWvfbKE"
   },
   "outputs": [],
   "source": [
    "# Final check before model training.  Test Validation or Train by changing the dataset\n",
    "\n",
    "#for batch_image, batch_masks in train_dataset.take(1):\n",
    "for batch_image, batch_masks in val_dataset.take(1):  \n",
    "    show_batch_image_masks(batch_image, batch_masks)\n",
    "    \n",
    "show_batch_image_masks([some_image], [some_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WboW5rmAh4yv"
   },
   "source": [
    "## Build  model\n",
    "- add and validate pretrained model as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6KnOf_oh4yw"
   },
   "outputs": [],
   "source": [
    "# Create any call backs for training...These are the most common.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_dice_coe', patience=3, verbose=1, min_lr=1e-6)\n",
    "#earlystopper = EarlyStopping(patience=6, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(parms.MODEL_PATH, monitor='val_dice_coe', verbose=1, mode=\"max\", save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, min_lr=1e-6)\n",
    "earlystopper = EarlyStopping(patience=6, verbose=1)\n",
    "checkpointer = ModelCheckpoint(parms.MODEL_PATH, monitor='val_loss', verbose=1, mode=\"min\", save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg1IuTqEh4y0"
   },
   "outputs": [],
   "source": [
    "# Create model and compile it\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D, Conv2DTranspose, Concatenate, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, Nadam, SGD\n",
    "########\n",
    "\n",
    "# https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/\n",
    "def combo_loss(y_true, y_pred):\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
    "        denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n",
    "        return tf.reshape(1 - numerator / denominator, (-1, 1, 1))\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True) + dice_loss(y_true, y_pred)\n",
    "\n",
    "K = tf.keras.backend\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / \\\n",
    "           (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def compile_model(parms, model):\n",
    "# sample_weights = tf.convert_to_tensor([0.1, .225, .225, .225, .225])\n",
    "\n",
    "    model.compile(\n",
    "        #loss='binary_crossentropy',\n",
    "        #loss=combo_loss,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=Adam(lr=0.00005),  #\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHJP9A8_lLnr"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNoWnLV6QyJS"
   },
   "outputs": [],
   "source": [
    "# train from empty seg model, comment out if loading existing model\n",
    "STARTING_MODEL_PATH = \"/content/drive/My Drive/GitHub/MachineLearning/2-KaggleSteelDefects/segmodel-256-800-c5-V01.h5\"\n",
    "model = load_model(STARTING_MODEL_PATH)\n",
    "print(\"Loaded: \", STARTING_MODEL_PATH)\n",
    "\n",
    "model = compile_model(parms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_AE9vRvh4y8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=parms.EPOCS, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[reduce_lr, earlystopper, checkpointer] \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndNSTyg5eZJt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sparse_SteelDefects_Segmentation_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
